{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste comparativo entre ELM, ELM dual com kernel polinomial e ELM dual com kernel RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "from numpy import linalg as LA\n",
    "from copy import deepcopy\n",
    "from scipy.spatial.distance import cdist\n",
    "import random\n",
    "from numba import jit, cuda\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    " \n",
    "file_path = 'output_final.txt'\n",
    "sys.stdout = open(file_path, \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L0_norm(w, threshold):\n",
    "    l0_norm = 0\n",
    "    for wi in w:\n",
    "        if abs(wi) > threshold:\n",
    "            l0_norm += 1\n",
    "    return l0_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_margin(K, y, alpha, b, norm_w, model):\n",
    "    margin = []\n",
    "    if model == \"RP-IMA dual\":\n",
    "        for i in range(y.shape[0]):\n",
    "            margin.append(y[i]*(np.dot(alpha * y, K[:, i]) + b)/norm_w)\n",
    "    elif model == \"ELM dual\" : \n",
    "        for i in range(y.shape[0]):\n",
    "            margin.append(y[i]*(np.dot(alpha, K[:, i]) + b)/norm_w)\n",
    "    if min(margin) >= 0:\n",
    "        return min(margin)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação ELM Dual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    sigmoid = 1.0/(1.0 + np.exp(-z))\n",
    "    return sigmoid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RP_IMA_dual(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, n_neurons, kernel='rbf', delta_margin=10e-3, gamma='scale', degree=3,\n",
    "                eta=0.1, max_fmp_updates=100000, IMA_iterations=20,\n",
    "                regularization_term=0, seed = 0):\n",
    "        self.n_neurons = n_neurons   # number of hidden neurons\n",
    "        self.kernel = kernel         # kernel type\n",
    "        self.gamma = gamma           # kernel param for RBF\n",
    "        self.degree = degree         # degree of polynomial kernel\n",
    "        self.delta_margin = delta_margin    # minimum margin increment\n",
    "        self.eta = eta\n",
    "        self.max_fmp_updates = max_fmp_updates\n",
    "        self.IMA_iterations = IMA_iterations\n",
    "        self.regularization_term = regularization_term # regularization parameter\n",
    "        self.threshold = 0.1\n",
    "        self.iterations = 1\n",
    "        self.seed = seed\n",
    "\n",
    "    # apply linear kernely * \n",
    "    @staticmethod\n",
    "    @jit(target_backend='cuda')\n",
    "    def linear_kernel(H):\n",
    "        return np.dot(H, np.transpose(H))\n",
    "\n",
    "    # apply plynomial kernel\n",
    "    @staticmethod\n",
    "    @jit(target_backend='cuda')\n",
    "    def polynomial_kernel(H, degree):\n",
    "        return (np.dot(H, np.transpose(H)))**degree\n",
    "\n",
    "    @staticmethod\n",
    "    def silverman_rule(data):\n",
    "        n = len(data)\n",
    "        std_data = np.std(data)\n",
    "        IQR = np.subtract(*np.percentile(data, [75, 25]))\n",
    "        h = 0.9 * np.min([std_data, IQR/1.34]) * n**(-0.2)\n",
    "        return h\n",
    "\n",
    "    @staticmethod\n",
    "    def rbf_kernel(H, gamma):\n",
    "        pairwise_dists = cdist(H, H, 'euclidean')\n",
    "        K = np.exp((-pairwise_dists**2)/(gamma**2))\n",
    "        return K\n",
    "\n",
    "\n",
    "    # Dual algorithm to training the fixed geometric mergin perceptron\n",
    "    @staticmethod\n",
    "    @jit(target_backend='cuda')\n",
    "    def dual_FMP_algorithm(K, y, alpha, b, eta, fixed_margin, max_fmp_updates):\n",
    "        stop = False\n",
    "        norm_w=0              # norm of w vector\n",
    "        fmp_epochs=0          # number of fmp epochs\n",
    "        fmp_norm_updates = 0  # number of updates of the norm of w\n",
    "\n",
    "        for i in range(K.shape[0]):\n",
    "            # Cálculo da norma do vetor W:\n",
    "            norm_w += alpha[i] * y[i] * np.dot(alpha * y , K[:, i])\n",
    "        norm_w = sqrt(norm_w)\n",
    "\n",
    "        while True:\n",
    "            # Checking stop condition\n",
    "            if stop:\n",
    "                convergence = 1\n",
    "                return convergence, fmp_norm_updates, fmp_epochs, norm_w, alpha, b\n",
    "            elif fmp_norm_updates > max_fmp_updates:\n",
    "                convergence = 0\n",
    "                return convergence, fmp_norm_updates, fmp_epochs, norm_w, alpha, b\n",
    "            else:\n",
    "                error = False\n",
    "                # subtract delta from alpha_i values before every new perceptron iteration ,\n",
    "                # making sure that the property alpha_i >= 0 is respected.\n",
    "                #for i in range(X.shape[0]):\n",
    "                #    self.alpha[i] = self.alpha[i] - self.delta\n",
    "                #    if self.alpha[i] < 0:\n",
    "                #        self.alpha[i] = 0\n",
    "                # Loop:\n",
    "                # Delta_b = 0\n",
    "                for i in range(K.shape[0]):\n",
    "                    Delta=0\n",
    "                    # Checking the margin criterion\n",
    "                    if((y[i] * np.dot(alpha, y * K[:, i])) <= fixed_margin * norm_w):\n",
    "                        fmp_norm_updates += 1\n",
    "                        # alpha update:\n",
    "                        previous_alpha_y = alpha[i]*y[i]\n",
    "                        if norm_w != 0:\n",
    "                            alpha = alpha * (1 - (eta*fixed_margin)/norm_w)\n",
    "                        alpha[i] += eta\n",
    "                        Delta = alpha[i]*y[i] - previous_alpha_y\n",
    "                        # Delta_b += Delta\n",
    "                        # norm update:\n",
    "                        norm_w = sqrt(norm_w**2 + Delta * np.dot(alpha, y * K[i, :]))\n",
    "                        error = True\n",
    "                        #b += eta * y[i]\n",
    "                norm_w=0\n",
    "                for i in range(K.shape[0]):\n",
    "                    # Cálculo da norma do vetor W:\n",
    "                    norm_w += alpha[i] * y[i] * np.dot(alpha * y , K[:, i])\n",
    "                norm_w = sqrt(norm_w)\n",
    "                if not error:\n",
    "                    stop = True\n",
    "                fmp_epochs+=1\n",
    "\n",
    "\n",
    "    def IM_algorithm(self, X, y):\n",
    "        #alpha = np.array([random.uniform(0, 1) for i in range(X.shape[0])]) #\n",
    "        alpha = np.ones((X.shape[0])) * 0.0001\n",
    "        self.fixed_margin = 0\n",
    "        t = 0\n",
    "        convergence = 1\n",
    "        self.updates = 0\n",
    "        iterations = 0\n",
    "        self.b=0  # bias term\n",
    "        # perceptron kernel matrix\n",
    "        if self.kernel == 'linear':\n",
    "            self.K = self.linear_kernel(X) + np.identity(X.shape[0]) * self.regularization_term\n",
    "        elif self.kernel == 'poly':\n",
    "            self.K = self.polynomial_kernel(X, self.degree) + np.identity(X.shape[0]) * self.regularization_term\n",
    "        elif self.kernel == 'rbf':\n",
    "            self.gamma = self.silverman_rule(X)\n",
    "            self.K = self.rbf_kernel(X, self.gamma) + np.identity(X.shape[0]) * self.regularization_term\n",
    "        \n",
    "        while convergence == 1 and t < self.IMA_iterations:\n",
    "            gamma1 = []\n",
    "            gamma2 = []\n",
    "            #print(f'{self.fixed_margin:1.30f}')\n",
    "            convergence, updates_, iterations, norm_w, alpha, b = self.dual_FMP_algorithm(self.K, y, alpha, self.b, self.eta, self.fixed_margin, self.max_fmp_updates)\n",
    "            self.updates += updates_\n",
    "            if convergence == 1 or t==0:\n",
    "                self.alpha = deepcopy(alpha)\n",
    "                self.b = 0#b\n",
    "                self.norm_w = norm_w\n",
    "                for i in range(0, y.shape[0]):\n",
    "                    aux = np.dot(self.alpha * y, self.K[:, i])\n",
    "                    if y[i] == 1:\n",
    "                        gamma1.append(y[i]*((aux + self.b)/norm_w))\n",
    "                    else:\n",
    "                        gamma2.append(y[i]*((aux + self.b)/norm_w))\n",
    "                gamma1 = np.array(gamma1)\n",
    "                gamma2 = np.array(gamma2)\n",
    "                gamma1 = gamma1[gamma1>=0]\n",
    "                gamma2 = gamma2[gamma2>=0]\n",
    "                min_gamma1 = 0\n",
    "                min_gamma2 = 0\n",
    "                if len(gamma1) > 0:\n",
    "                    min_gamma1 = min(gamma1)\n",
    "                if len(gamma2) > 0:\n",
    "                    min_gamma2 = min(gamma2)\n",
    "                self.fixed_margin = max([(min_gamma1 + min_gamma2)/2, (1+self.delta_margin)*self.fixed_margin])\n",
    "                #print(f\"IMA: {t}\")\n",
    "            t += 1\n",
    "        self.iterations = t\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 1 - Adding polarization term\n",
    "        X_new = np.ones((X.shape[0], X.shape[1]+1))\n",
    "        X_new[:,1:] = X\n",
    "        n = X_new.shape[1]\n",
    "        # 2 - Geração de pesos aleatórios\n",
    "        random.seed(self.seed)\n",
    "        self.Z = np.array([random.uniform(-0.5, 0.5) for i in range(n*self.n_neurons)]).reshape(n, self.n_neurons)\n",
    "        # 3 - Aplicação da função de ativação\n",
    "        self.H = sigmoid(np.dot(X_new, self.Z))\n",
    "        # 4 - IMA Dual:\n",
    "        self.y_train = y\n",
    "        self.IM_algorithm(self.H, y)\n",
    "        # Cálculo do vetor beta para kernel linear\n",
    "        if self.kernel == 'linear':\n",
    "            for i in range(0, y.shape[0]):\n",
    "                self.beta = np.dot(self.alpha * y, self.H)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_new = np.ones((X.shape[0], X.shape[1]+1))\n",
    "        X_new[:,1:] = X\n",
    "        H_ = sigmoid(np.dot(X_new, self.Z))\n",
    "\n",
    "        if self.kernel == 'linear':\n",
    "            y_predicted = np.dot(H_, self.beta)\n",
    "        elif self.kernel == 'poly':\n",
    "            y_predicted = np.zeros(X.shape[0])\n",
    "            for j in range(X.shape[0]):\n",
    "                for i in range(X.shape[0]):\n",
    "                    y_predicted[j] += self.alpha[i] * self.y_train[i] * (np.dot(self.H[i], H_[j]) + 1)**self.degree\n",
    "        elif self.kernel == 'rbf':\n",
    "            y_predicted = np.zeros(X.shape[0])\n",
    "            for j in range(H_.shape[0]):\n",
    "                for i in range(self.H.shape[0]):\n",
    "                    y_predicted[j] += self.alpha[i] * self.y_train[i] * np.exp((-np.linalg.norm(H_[j] - self.H[i])**2)/self.gamma**2)\n",
    "                #y_predicted[j] += self.b\n",
    "        y_predicted[y_predicted>=0] = 1\n",
    "        y_predicted[y_predicted<0] = -1\n",
    "        return y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação ELM tradicional (forma Dual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELM_dual(BaseEstimator, ClassifierMixin):\n",
    "         \n",
    "    def __init__(self, n_neurons, kernel='rbf', regularization_term= 0, degree=3, seed = 0):\n",
    "        self.n_neurons = n_neurons\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.regularization_term = regularization_term\n",
    "        self.iterations = 1\n",
    "        self.seed = seed\n",
    "        \n",
    "    # apply linear kernel\n",
    "    @staticmethod\n",
    "    @jit(target_backend='cuda')\n",
    "    def linear_kernel(H):\n",
    "        return np.dot(H, np.transpose(H))\n",
    "\n",
    "    # apply plynomial kernel\n",
    "    @staticmethod\n",
    "    @jit(target_backend='cuda')\n",
    "    def polynomial_kernel(H, degree):\n",
    "        return (np.dot(H, np.transpose(H)))**degree\n",
    "\n",
    "    @staticmethod\n",
    "    def silverman_rule(data):\n",
    "        n = len(data)\n",
    "        std_data = np.std(data)\n",
    "        IQR = np.subtract(*np.percentile(data, [75, 25]))\n",
    "        h = 0.9 * np.min([std_data, IQR/1.34]) * n**(-0.2)\n",
    "        return h\n",
    "\n",
    "    @staticmethod\n",
    "    def rbf_kernel(H, gamma):\n",
    "        pairwise_dists = cdist(H, H, 'euclidean')\n",
    "        K = np.exp((-pairwise_dists**2)/(gamma**2))\n",
    "        return K\n",
    "               \n",
    "    def fit(self, X, y):\n",
    "        # 1 - Adding polarization term \n",
    "        X_new = np.ones((X.shape[0], X.shape[1]+1))\n",
    "        X_new[:,1:] = X\n",
    "        n = X_new.shape[1]\n",
    "        # 2 - Geração de pesos aleatórios\n",
    "        random.seed(self.seed)\n",
    "        self.Z = np.array([random.uniform(-0.5, 0.5) for i in range(n*self.n_neurons)]).reshape(n, self.n_neurons)\n",
    "        # 3 - Aplicação da função de ativação\n",
    "        self.H = sigmoid(np.dot(X_new, self.Z))\n",
    "        # 4 - Aplicação do kernel\n",
    "        if self.kernel == 'linear':\n",
    "            self.K = self.linear_kernel(self.H)\n",
    "        elif self.kernel == 'poly':\n",
    "            self.K = self.polynomial_kernel(self.H, self.degree)\n",
    "        elif self.kernel == 'rbf':\n",
    "            self.gamma = self.silverman_rule(self.H)\n",
    "            self.K = self.rbf_kernel(self.H, self.gamma) + np.identity(self.H.shape[0]) * self.regularization_term\n",
    "        # 5 - Cálculo de alpha:\n",
    "        self.alpha = np.dot(np.linalg.pinv(self.K, hermitian=True), y)\n",
    "        # 6 Cálculo do vetor beta\n",
    "        if self.kernel == 'linear':\n",
    "            self.beta = np.dot(np.transpose(self.H), self.alpha)\n",
    "        self.norm_w = 0\n",
    "        for i in range(self.K.shape[0]):\n",
    "            # Cálculo da norma do vetor W:\n",
    "            self.norm_w += self.alpha[i] * np.dot(self.alpha, self.K[:, i])\n",
    "        self.norm_w = sqrt(self.norm_w)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        X_new = np.ones((X.shape[0], X.shape[1]+1))\n",
    "        X_new[:,1:] = X\n",
    "        H_ = sigmoid(np.dot(X_new, self.Z))\n",
    "        if self.kernel == 'linear':\n",
    "            y_predicted = np.dot(H_, self.beta)\n",
    "        elif self.kernel == 'poly':\n",
    "            y_predicted = np.zeros(X.shape[0])\n",
    "            for j in range(H_.shape[0]):\n",
    "                for i in range(self.H.shape[0]):\n",
    "                    y_predicted[j] += self.alpha[i] * (np.dot(self.H[i], H_[j]) + 1)**self.degree\n",
    "        elif self.kernel == 'rbf':\n",
    "            y_predicted = np.zeros(X.shape[0])\n",
    "            for j in range(H_.shape[0]):\n",
    "                for i in range(self.H.shape[0]):\n",
    "                    y_predicted[j] += self.alpha[i] * np.exp((-np.linalg.norm(H_[j] - self.H[i])**2)/self.gamma**2)\n",
    "        y_predicted[y_predicted>=0] = 1\n",
    "        y_predicted[y_predicted<0] = -1\n",
    "        return y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para realizar a testagem dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cross_validation(model, X, y, params={}, n_splits=10, seed=1, model_title=\"Model\"):\n",
    "    k_fold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    accuracy = np.zeros(n_splits)\n",
    "    train_accuracy = np.zeros(n_splits)\n",
    "    precision = np.zeros(n_splits)\n",
    "    recall = np.zeros(n_splits)\n",
    "    f1 = np.zeros(n_splits)\n",
    "    iterations = np.zeros(n_splits)\n",
    "    seed = [10, 22, 52, 59, 1, 21, 99, 85, 74, 69]\n",
    "    \n",
    "    norm_L0_1 = np.zeros(n_splits)\n",
    "    norm_L0_2 = np.zeros(n_splits)\n",
    "    norm_L0_3 = np.zeros(n_splits)\n",
    "    norm_L0_4 = np.zeros(n_splits)\n",
    "    \n",
    "    margin = np.zeros(n_splits)\n",
    "    \n",
    "    idx = 0  \n",
    "    for train_indices, test_indices in k_fold.split(X, y):\n",
    "        X_train = X[train_indices]\n",
    "        X_test = X[test_indices]\n",
    "        y_train = y[train_indices]\n",
    "        y_test = y[test_indices]\n",
    "        \n",
    "        # Model apply:\n",
    "        params['seed'] = seed[idx]\n",
    "        clf = model(**params)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "\n",
    "        # Calculating accuracy, precision, reall and F1 score:\n",
    "        precision[idx] = precision_score(y_test, y_pred)\n",
    "        recall[idx] = recall_score(y_test, y_pred)\n",
    "        accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        train_accuracy[idx] = accuracy_score(y_train, y_pred_train)\n",
    "        f1[idx] = f1_score(y_test, y_pred)\n",
    "        margin[idx] = compute_margin(clf.K, y_train, clf.alpha, 0, clf.norm_w, model_title)\n",
    "        \n",
    "        alpha = clf.alpha\n",
    "        norm = LA.norm(alpha, ord=2)\n",
    "        alpha = alpha/norm\n",
    "        norm_L0_1[idx] = L0_norm(alpha, 0.1 * alpha.max())\n",
    "        norm_L0_2[idx] = L0_norm(alpha, 0.01 * alpha.max())\n",
    "        norm_L0_3[idx] = L0_norm(alpha, 0.001 * alpha.max())\n",
    "        norm_L0_4[idx] = L0_norm(alpha, 0.0001 * alpha.max())\n",
    "        iterations[idx] = clf.iterations\n",
    "        idx +=1\n",
    "        \n",
    "    mean_accuracy = '{:.4f}'.format(accuracy.mean()) + \"+/-\" + '{:.4f}'.format(accuracy.std())\n",
    "    mean_accuracy_train = '{:.4f}'.format(train_accuracy.mean()) + \"+/-\" + '{:.4f}'.format(train_accuracy.std())\n",
    "    mean_precision ='{:.4f}'.format(precision.mean()) + \"+/-\" + '{:.4f}'.format(precision.std())\n",
    "    mean_recall = '{:.4f}'.format(recall.mean()) + \"+/-\" + '{:.4f}'.format(recall.std())\n",
    "    mean_f1 = '{:.4f}'.format(f1.mean()) + \"+/-\" + '{:.4f}'.format(f1.std())\n",
    "    mean_margin = '{:E}'.format(margin.mean()) + \"+/-\" + '{:E}'.format(margin.std())\n",
    "    \n",
    "    print(f\"******************** {model_title} ********************\") \n",
    "    print(\"Mean Train Accuracy: \" + mean_accuracy_train) \n",
    "    print(\"Mean Accuracy: \" + mean_accuracy) \n",
    "    print(\"Mean Precision: \" + mean_precision) \n",
    "    print(\"Mean Recall: \" + mean_recall) \n",
    "    print(\"Mean F1 Score: \" + mean_f1) \n",
    "    print(\"Mean Margin: \" + mean_margin) \n",
    "    print(\"Norm L0 (10%): \" + '{:.9f}'.format(norm_L0_1.mean())+ \"+/-\" + '{:.9f}'.format(norm_L0_1.std()))\n",
    "    print(\"Norm L0 (1%): \" + '{:.9f}'.format(norm_L0_2.mean())+ \"+/-\" + '{:.9f}'.format(norm_L0_2.std()))\n",
    "    print(\"Norm L0 (0.1%): \" + '{:.9f}'.format(norm_L0_3.mean())+ \"+/-\" + '{:.9f}'.format(norm_L0_3.std()))\n",
    "    print(\"Norm L0 (0.01%): \" + '{:.9f}'.format(norm_L0_4.mean())+ \"+/-\" + '{:.9f}'.format(norm_L0_4.std()))\n",
    "    print(f\"Iterations: {iterations.mean()}\") \n",
    "    print('\\n\\n')\n",
    "    return mean_accuracy, mean_precision, mean_recall, mean_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model, X, y, params={}, gridsearch_params={}, model_title=\"Model\"):\n",
    "    accuracy = list(np.empty(3))\n",
    "    precision = list(np.empty(3))\n",
    "    recall = list(np.empty(3))\n",
    "    f1 = list(np.empty(3))\n",
    "    N = 1000 if X.shape[0]>=1000 else X.shape[0]\n",
    "    X = preprocessing.normalize(X, axis=0)\n",
    "    for idx, n_neurons in enumerate([N, int(N/2), int(N/3)]):\n",
    "        params['n_neurons'] = n_neurons\n",
    "        model_params = params\n",
    "        if gridsearch_params != {}:\n",
    "            clf = GridSearchCV(model(**params), gridsearch_params , scoring='accuracy', refit=True, cv=10, verbose=0)\n",
    "            # Run fit with all sets of parameters.\n",
    "            clf.fit(X, y)\n",
    "            # Results of Grid Search\n",
    "            model_params = {**params, **clf.best_params_}\n",
    "        print(model_params)\n",
    "        accuracy[idx], precision[idx], recall[idx], f1[idx] = apply_cross_validation(model, X, y, model_params, n_splits=10, model_title=model_title)\n",
    "    return  accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_elm = {}\n",
    "precision_elm = {} \n",
    "recall_elm = {}\n",
    "f1_elm = {}\n",
    "accuracy_elm_ima = {}\n",
    "precision_elm_ima = {} \n",
    "recall_elm_ima = {}\n",
    "f1_elm_ima = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11126/3352473659.py:57: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 1d, C), array(float64, 1d, A))\n",
      "  norm_w += alpha[i] * y[i] * np.dot(alpha * y , K[:, i])\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "# setosa - 0, versicolor - 1, virginica - 2  \n",
    "y = iris.target \n",
    "# O problema agora possui apenas as classes y=-1 e y=1\n",
    "y[y>0] = 1\n",
    "y[y==0] = -1\n",
    "\n",
    "dataset = 'IRI' \n",
    "print(dataset) \n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]} \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, gridsearch_params=grid_params, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf'}\n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dataset = pd.read_csv('~/Documents/UFMG/Mastering/article/data sets/synthetic_dataset/synthetic_control.data', sep=\"\\s+\",  header=None, engine='python')\n",
    "X = synthetic_dataset.to_numpy()\n",
    "y = np.concatenate((np.ones(100), np.ones(200)*-1, np.ones(100), np.ones(100)*-1,np.ones(100)))\n",
    "\n",
    "dataset = 'SYN' \n",
    "print(dataset) \n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]} \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, gridsearch_params=grid_params, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf'}\n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ionosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ionosphere_dataset = pd.read_csv('~/Documents/UFMG/Mastering/article/data sets/Ionosphere/ionosphere.data', names=list(range(0,35)), sep=',')\n",
    "y = ionosphere_dataset[34].to_numpy()\n",
    "X = ionosphere_dataset.drop([34], axis='columns').to_numpy()\n",
    "y[np.where(y=='g')] = 1\n",
    "y[np.where(y=='b')] = -1\n",
    "y = np.array(y.tolist())\n",
    "\n",
    "dataset = 'ION' \n",
    "print(dataset) \n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]} \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, gridsearch_params=grid_params, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf'}\n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataset = pd.read_csv('~/Documents/UFMG/Mastering/article/data sets/wine/wine.data', names=['Class', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315', 'Proline'])\n",
    "# convert to array\n",
    "y = wine_dataset[['Class']].to_numpy()\n",
    "X = wine_dataset.drop(\"Class\",axis='columns').to_numpy()\n",
    "y[np.where(y==3)] = 1\n",
    "y[np.where(y==2)] = -1\n",
    "y = y.flatten()\n",
    "\n",
    "dataset = 'WIN' \n",
    "print(dataset)\n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]} \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, gridsearch_params=grid_params, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf'}\n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wdbc_dataset = pd.read_csv('~/Documents/UFMG/Mastering/article/data sets/WDBC/wdbc.data', names=list(range(0,32)))\n",
    "# convert to array\n",
    "y = wdbc_dataset[1].to_numpy()\n",
    "X = wdbc_dataset.drop([0, 1],axis='columns').to_numpy()\n",
    "y[np.where(y=='B')] = 1\n",
    "y[np.where(y=='M')] = -1\n",
    "y = np.array(y.tolist())\n",
    "\n",
    "dataset = 'WDBC' \n",
    "print(dataset) \n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]} \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, gridsearch_params=grid_params, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf'}\n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sonar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar_dataset = pd.read_csv('~/Documents/UFMG/Mastering/article/data sets/sonar/sonar.all-data', names=list(range(0,61)), sep=',')\n",
    "y = sonar_dataset[60].to_numpy()\n",
    "X = sonar_dataset.drop([60], axis='columns').to_numpy()\n",
    "y[np.where(y=='R')] = 1\n",
    "y[np.where(y=='M')] = -1\n",
    "y = np.array(y.tolist())\n",
    "dataset = 'SON' \n",
    "print(dataset) \n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]} \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, gridsearch_params=grid_params, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf'}\n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pima_dataset = pd.read_csv('~/Documents/UFMG/Mastering/article/data sets/diabetes/diabetes.csv', sep=\",\", engine='python')\n",
    "y = pima_dataset['Outcome'].to_numpy()\n",
    "X = pima_dataset.drop(['Outcome'], axis='columns').to_numpy()\n",
    "y[np.where(y==0)] = -1\n",
    "y = np.array(y.tolist())\n",
    "\n",
    "dataset = 'DIA' \n",
    "print(dataset) \n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]} \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, gridsearch_params=grid_params, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf'}\n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "statlog_dataset = pd.read_csv('~/Documents/UFMG/Mastering/article/data sets/statlog/heart.dat', sep=\" \", header=None, engine='python')\n",
    "y = statlog_dataset[13].to_numpy()\n",
    "X = statlog_dataset.drop([13], axis='columns').to_numpy()\n",
    "y[np.where(y==2)] = -1\n",
    "y = np.array(y.tolist())\n",
    "\n",
    "dataset = 'HEA' \n",
    "print(dataset) \n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]} \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, gridsearch_params=grid_params, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf'}\n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Banknote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in banknote authentication set\n",
    "banknotes = pd.read_csv('~/Documents/UFMG/Mastering/article/data sets/banknote/data_banknote_authentication.txt', names=['variance', 'skewness', 'curtosis', 'entropy', 'class'])\n",
    "# convert to array\n",
    "X = banknotes[['variance', 'skewness', 'curtosis', 'entropy']].to_numpy()\n",
    "y = banknotes[['class']].to_numpy()\n",
    "y[np.where(y==0)] = -1\n",
    "y = y.flatten()\n",
    "\n",
    "dataset = 'BAN' \n",
    "print(dataset) \n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]} \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, gridsearch_params=grid_params, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf'}\n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mammographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mammo = pd.read_csv('~/Documents/UFMG/Mastering/article/data sets/mammographic/mammographic_masses.data', sep=\",\", header=None, engine='python')\n",
    "mammo = mammo.replace(\"?\", np.nan)\n",
    "mammo = mammo.dropna()\n",
    "y = mammo[5].to_numpy()\n",
    "X = mammo.drop([5], axis='columns').to_numpy()\n",
    "y[np.where(y==0)] = -1\n",
    "y = np.array(y.tolist())\n",
    "\n",
    "dataset = 'MAM' \n",
    "print(dataset) \n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]} \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, gridsearch_params=grid_params, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf'}\n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haberman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "haberman = pd.read_csv('~/Documents/UFMG/Mastering/article/data sets/haberman/haberman.data', sep=\",\", header=None, engine='python')\n",
    "y = haberman[3].to_numpy()\n",
    "X = haberman.drop([3], axis='columns').to_numpy()\n",
    "y[np.where(y==2)] = -1\n",
    "y = np.array(y.tolist())\n",
    "\n",
    "dataset = 'HAB' \n",
    "print(dataset) \n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]} \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, gridsearch_params=grid_params, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf'}\n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfusion = pd.read_csv('~/Documents/UFMG/Mastering/article/data sets/transfusion/transfusion.data', sep=\",\", engine='python')\n",
    "y = transfusion[\"whether he/she donated blood in March 2007\"].to_numpy()\n",
    "X = transfusion.drop([\"whether he/she donated blood in March 2007\"], axis='columns').to_numpy()\n",
    "y[np.where(y==0)] = -1\n",
    "y = np.array(y.tolist())\n",
    "\n",
    "dataset = 'TRA' \n",
    "print(dataset) \n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]} \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, gridsearch_params=grid_params, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf'}\n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Australian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "australian = pd.read_csv('~/Documents/UFMG/Mastering/article/data sets/australian_credit/australian.dat', header=None, sep=\" \", engine='python')\n",
    "australian = australian.replace(\"?\", np.nan)\n",
    "australian = australian.dropna()\n",
    "y = australian[14].to_numpy()\n",
    "X = australian.drop([14], axis='columns').to_numpy()\n",
    "y[np.where(y==0)] = -1\n",
    "y = np.array(y.tolist())\n",
    "\n",
    "dataset = 'AUS' \n",
    "print(dataset) \n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]} \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, gridsearch_params=grid_params, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf'}\n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast = pd.read_csv('~/Documents/UFMG/Mastering/article/data sets/breast/breast.data', header=None, sep=\",\", engine='python')\n",
    "breast = breast.replace(\"?\", np.nan)\n",
    "breast = breast.dropna()\n",
    "y = breast[10].to_numpy()\n",
    "X = breast.drop([0, 10], axis='columns').to_numpy()\n",
    "y[np.where(y==4)] = 1\n",
    "y[np.where(y==2)] = -1\n",
    "y = np.array(y.tolist())\n",
    "\n",
    "dataset = 'BRE' \n",
    "print(dataset) \n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]} \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, gridsearch_params=grid_params, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf'}\n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset:\n",
    "headers = [\"Id\", \"RI\", \"Na\", \"Mg\", \"Al\", \"Si\", \"K\", \"Ca\", \"Ba\", \"Fe\", \"Class\"]\n",
    "df = pd.read_csv(\"~/Documents/UFMG/Graduation/10/Reconhecimento de padrões/list/pattern-recognition-exercises/list_5/databases/glass.csv\", names = headers)\n",
    "X = df.drop(\"Class\", axis=1)\n",
    "X = X.drop(\"Id\", axis=1)\n",
    "y = df[\"Class\"]\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "y[np.where(y>1)] = -1\n",
    "\n",
    "dataset = 'GLA' \n",
    "print(dataset) \n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]} \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, gridsearch_params=grid_params, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf'}\n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_csv('~/Documents/UFMG/Mastering/article/data sets/spam/spambase.data', header=None, sep=\",\", engine='python')\n",
    "y = spam[57].to_numpy()\n",
    "X = spam.drop([57], axis='columns').to_numpy()\n",
    "y[np.where(y==0)] = -1\n",
    "y = np.array(y.tolist())\n",
    "\n",
    "dataset = 'SPA' \n",
    "print(dataset) \n",
    "params = {'regularization_term': 1} \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, params, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf', 'regularization_term': 10}\n",
    "grid_params = {}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mushroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/Documents/UFMG/Mastering/article/data sets/Mushroom/agaricus-lepiota.data', delimiter =',', header=None)\n",
    "df = df.replace(\"?\", np.nan) \n",
    "df = df.dropna() \n",
    "y = df[0].to_numpy()\n",
    "X = df.drop([0], axis='columns')\n",
    "X = pd.get_dummies(X).to_numpy()\n",
    "y[np.where(y=='e')] = -1\n",
    "y[np.where(y=='p')] = 1\n",
    "y = np.array(y.tolist())\n",
    "\n",
    "dataset = 'MUS' \n",
    "print(dataset) \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf', 'regularization_term': 0}\n",
    "grid_params = {}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_dataset = pd.read_csv('~/Documents/UFMG/Mastering/article/data sets/robot/lp4_data.csv', delimiter =',')\n",
    "X = robot_dataset.to_numpy().reshape([117,90])\n",
    "y = np.concatenate((np.ones(24), np.ones(117-24)*-1))\n",
    "\n",
    "dataset = 'ROB' \n",
    "print(dataset) \n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]} \n",
    "model = ELM_dual\n",
    "accuracy_elm[dataset], precision_elm[dataset], recall_elm[dataset], f1_elm[dataset] = run_test(model, X, y, gridsearch_params=grid_params, model_title=\"ELM dual\")\n",
    "\n",
    "params = {'kernel' : 'rbf'}\n",
    "grid_params = {'regularization_term':[0, 0.001, 0.01, 1, 10]}\n",
    "model = RP_IMA_dual\n",
    "accuracy_elm_ima[dataset], precision_elm_ima[dataset], recall_elm_ima[dataset], f1_elm_ima[dataset] = run_test(model, X, y, params, grid_params, model_title=\"RP-IMA dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabela de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = pd.DataFrame([[\"Dual ELM (RBF)\", \"N\"], \n",
    "                             [\"Dual ELM (RBF)\", \"N/2\"], \n",
    "                             [\"Dual ELM (RBF)\", \"N/3\"],\n",
    "                             [\"Dual RP-IMA (RBF)\", \"N\"], \n",
    "                             [\"Dual RP-IMA (RBF)\", \"N/2\"], \n",
    "                             [\"Dual RP-IMA (RBF)\", \"N/3\"]],\n",
    "                             columns=[\"Data sets\", \"\"])\n",
    "\n",
    "datasets = ['IRI', 'SYN', 'ION', 'BAN', 'WIN', 'WDBC', 'SON', 'DIA', 'HEA', \n",
    "            'MAM', 'HAB', 'TRA', 'AUS', 'BRE', 'GLA', 'MUS', 'ROB', 'SPA']\n",
    "\n",
    "rows=[]\n",
    "for dataset in datasets:\n",
    "    rows.append([accuracy_elm[dataset][0], accuracy_elm[dataset][1], accuracy_elm[dataset][2], accuracy_elm_ima[dataset][0], accuracy_elm_ima[dataset][1], accuracy_elm_ima[dataset][2]])\n",
    "\n",
    "columns = pd.MultiIndex.from_frame(column_names)\n",
    "df = pd.DataFrame(rows, columns=columns, index=datasets)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11126/2350534188.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(df.to_latex())\n"
     ]
    }
   ],
   "source": [
    "print(df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset:\n",
    "headers = [\"Id\", \"RI\", \"Na\", \"Mg\", \"Al\", \"Si\", \"K\", \"Ca\", \"Ba\", \"Fe\", \"Class\"]\n",
    "df = pd.read_csv(\"~/Documents/UFMG/Graduation/10/Reconhecimento de padrões/list/pattern-recognition-exercises/list_5/databases/glass.csv\", names = headers)\n",
    "X = df.drop(\"Class\", axis=1)\n",
    "X = X.drop(\"Id\", axis=1)\n",
    "y = df[\"Class\"]\n",
    "X = X.to_numpy()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "haberman = pd.read_csv('~/Documents/UFMG/Mastering/article/data sets/haberman/haberman.data', sep=\",\", header=None, engine='python')\n",
    "y = haberman[3].to_numpy()\n",
    "X = haberman.drop([3], axis='columns').to_numpy()\n",
    "y[np.where(y==2)] = -1\n",
    "y = np.array(y.tolist())\n",
    "\n",
    "clf = RP_IMA_dual(n_neurons=100, eta=0.1, regularization_term=0, delta_margin=10^-1, IMA_iterations=200, max_fmp_updates=10000, seed=1)\n",
    "m = clf.fit(X, y)\n",
    "\n",
    "print(accuracy_score(y, clf.predict(X)))\n",
    "\n",
    "\n",
    "clf2 = ELM_dual(100, seed=1, regularization_term=0)\n",
    "clf2.fit(X, y)\n",
    "\n",
    "print(accuracy_score(y, clf2.predict(X)))\n",
    "\n",
    "m = compute_margin(clf.K, y, clf2.alpha, 0, clf2.norm_w, \"ELM dual\")\n",
    "print(m)\n",
    "m = compute_margin(clf.K, y, clf.alpha, 0, clf.norm_w, \"RP-IMA dual\")\n",
    "print(m)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "k39",
   "language": "python",
   "name": "k39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
