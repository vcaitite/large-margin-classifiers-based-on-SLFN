{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyP1G9r0kDO2"
   },
   "source": [
    "# Incremental margin algorithm for large margin classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N1mMqyF5kDO6"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, auc\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_digits\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "from numpy import linalg as LA\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1YAuDbBkDO-"
   },
   "source": [
    "## Calculating the margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eZvyTFl_kDO_"
   },
   "outputs": [],
   "source": [
    "def compute_margin(X, y, w, b):\n",
    "    margin = []\n",
    "    for i in range(y.shape[0]):\n",
    "        margin.append((y[i]*(np.dot(X[i,:], w)+b))/sqrt(sum(w**2))) \n",
    "    if min(margin) >= 0:\n",
    "        return min(margin)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L0 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L0_norm(w, threshold):\n",
    "    l0_norm = 0\n",
    "    for wi in w:\n",
    "        if abs(wi) > threshold:\n",
    "            l0_norm += 1\n",
    "    return l0_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mQ-uGN3kDPB"
   },
   "source": [
    "# ELM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    sigmoid = 1.0/(1.0 + np.exp(-z))\n",
    "    return sigmoid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELM with IM P inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class ELM_IMA(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    # Inicialization of important parameters \n",
    "    def __init__(self, n_neurons, eta=0.1, lambda_param=0.01, delta_margin=10^-3,\n",
    "                 IMA_iterations=10, max_updates=10000, threshold=0.1):\n",
    "        self.n_neurons = n_neurons              # Neurons of hidden layer osf ELM\n",
    "        self.eta = eta                          # Learning rate\n",
    "        self.lambda_param = lambda_param        # Param important of soft margin\n",
    "        self.delta_margin = delta_margin        # (1 + delta_margin) * fixed margin defines the minimum next margin of IMA\n",
    "        self.IMA_iterations = IMA_iterations    # Maximum number of iterations of IMA\n",
    "        self.max_updates = max_updates          # Maximum number of updates in one execution of FMP\n",
    "        self.w = np.array([])                   # Vector of weights of the last layer of the ELM obtained after the training of the IMA\n",
    "        self.w_elm = np.array([])               # Vector of weights of the last layer of the ELM obtained after the normal training of ELM\n",
    "        self.H = np.array([])                   # H matrix of ELM (obtained with training data)\n",
    "        self.Z = np.array([])                   # Z matrix of ELM\n",
    "        self.b = 0\n",
    "        self.threshold = threshold\n",
    "\n",
    "    # Fixed Margin Algorithm    \n",
    "    def FMP_algorithm(self, X, y, w_init, b_init, fixed_margin, idx, s):\n",
    "        t = 0\n",
    "        iterations = 0\n",
    "        w = w_init\n",
    "        b = b_init\n",
    "        w_norm_1 = LA.norm(w, ord=1)\n",
    "        last_t = -1\n",
    "        lambda_t = 0\n",
    "        alpha = np.zeros((X.shape[0]))\n",
    "        while True:\n",
    "            last_t = t\n",
    "            e=0\n",
    "            for k in range(0, y.shape[0]):\n",
    "                i = int(idx[k])\n",
    "                if(y[i]*(np.dot(X[i,:], w)+b) <= fixed_margin * w_norm_1 - self.lambda_param * alpha[i]):\n",
    "                    if w_norm_1 != 0:\n",
    "                        lambda_t = 1 - (self.eta*fixed_margin)/w_norm_1\n",
    "                    else:\n",
    "                        lambda_t = 1\n",
    "                    alpha = alpha * lambda_t\n",
    "                    alpha[i] = alpha[i] + self.eta    \n",
    "                    w = w - self.eta * (fixed_margin * np.sign(w) - y[i] * X[i,:])\n",
    "                    w_norm_1 = LA.norm(w, ord=1)\n",
    "                    b = b + self.eta*y[i]\n",
    "                    t += 1\n",
    "                    e += 1\n",
    "                    if k > s:\n",
    "                        s += 1\n",
    "                        j = s\n",
    "                    else:\n",
    "                        j=e\n",
    "                    idx[k], idx[j] = idx[j], idx[k]\n",
    "            iterations += 1\n",
    "            if (t > self.max_updates or last_t == t):\n",
    "                break\n",
    "        if t<= self.max_updates:\n",
    "            convergence=1\n",
    "        else:\n",
    "            convergence=0\n",
    "        return w, b, convergence, t, iterations, idx, s\n",
    "\n",
    "    # IMA Algorithm\n",
    "    def IM_algorithm(self, X, y):\n",
    "        self.w = np.zeros(self.H.shape[1])\n",
    "        self.ws = [] \n",
    "        self.bs = [] \n",
    "        self.ws.append(self.w)\n",
    "        self.bs.append(self.b)\n",
    "        fixed_margin = 0#compute_margin(X, y, self.w_elm, self.b)\n",
    "        t = 0\n",
    "        convergence = 1\n",
    "        updates=0\n",
    "        iterations=0\n",
    "        margin=[]\n",
    "        margin.append(fixed_margin)\n",
    "        idx = np.linspace(0, y.shape[0]-1, y.shape[0])\n",
    "        s=0\n",
    "        l=0\n",
    "        while convergence==1 and t<self.IMA_iterations:\n",
    "            w, b, convergence, updates_, iterations_, idx, s = self.FMP_algorithm(X, y, self.w, self.b, fixed_margin, idx, s)\n",
    "            if convergence == 1:\n",
    "                self.w = w\n",
    "                self.b = b\n",
    "                self.ws.append(self.w)\n",
    "                self.bs.append(self.b)\n",
    "            updates += updates_\n",
    "            iterations += iterations_\n",
    "            norm_w = LA.norm(w, ord=1)\n",
    "            gamma1 = []\n",
    "            gamma2 = []\n",
    "            for i in range(0, y.shape[0]):\n",
    "                if y[i] == 1:\n",
    "                    gamma1.append((y[i]*(np.dot(X[i], self.w)+self.b))/norm_w)\n",
    "                else:\n",
    "                    gamma2.append((y[i]*(np.dot(X[i], self.w)+self.b))/norm_w)\n",
    "            gamma1 = np.array(gamma1)\n",
    "            gamma2 = np.array(gamma2)\n",
    "            gamma1 = gamma1[gamma1>=0]\n",
    "            gamma2 = gamma2[gamma2>=0]\n",
    "            if len(gamma1) == 0:\n",
    "                min_gamma1 = 0\n",
    "            else:\n",
    "                min_gamma1 = min(gamma1)\n",
    "            if len(gamma2) == 0:\n",
    "                min_gamma2 = 0\n",
    "            else:\n",
    "                min_gamma2 = min(gamma2)\n",
    "            fixed_margin = max([(min_gamma1 + min_gamma2)/2, (1+self.delta_margin)*fixed_margin])\n",
    "            #margin.append(compute_margin(X, y, self.w, self.b))\n",
    "            t += 1\n",
    "            l0_norm = 0\n",
    "            remove = []\n",
    "            for i in range(len(self.w)):\n",
    "                if abs(self.w[i]) < self.threshold * self.w.max():\n",
    "                    remove.append(i)\n",
    "            self.w = np.delete(self.w, remove)\n",
    "            self.Z = np.delete(self.Z, remove, axis=1)\n",
    "            self.H = np.delete(self.H, remove, axis=1)\n",
    "            X = np.delete(X, remove, axis=1)\n",
    "        return t, updates, iterations\n",
    "\n",
    "    # Function that manage the training of IMA ELM\n",
    "    def fit(self, X, y, Z=[]):\n",
    "        X_new = np.ones((X.shape[0], X.shape[1]+1))\n",
    "        X_new[:,1:] = X\n",
    "        X = X_new\n",
    "        n = X.shape[1]\n",
    "        if len(Z) == 0:\n",
    "            self.Z = np.array([random.uniform(-1, 1) for i in range(n*self.n_neurons)]).reshape(n, self.n_neurons)\n",
    "        else:\n",
    "            self.Z = Z\n",
    "        self.H = sigmoid(np.dot(X, self.Z))\n",
    "        #w = np.dot(np.linalg.pinv(self.H), y)  \n",
    "        #self.w_elm = w.reshape((w.shape[0],))\n",
    "        iterations_IMA, updates, iterations = self.IM_algorithm(self.H, y) \n",
    "        return iterations_IMA, updates, iterations\n",
    "            \n",
    "    # Function to apply IMA ELM model\n",
    "    def predict(self, X, use_IMA_w=True):\n",
    "        X_new = np.ones((X.shape[0], X.shape[1]+1))\n",
    "        X_new[:,1:] = X\n",
    "        H = sigmoid(np.dot(X_new, self.Z))\n",
    "        if use_IMA_w == True:\n",
    "            y_predicted = np.sign(np.dot(H, self.w) + self.b)\n",
    "        #else:\n",
    "        #    y_predicted = np.sign(np.dot(H,  self.w_elm))\n",
    "        y_predicted[y_predicted==0]=-1\n",
    "        return y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGgtVO-SkDPH"
   },
   "source": [
    "## Commom ELM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hQdXLRz_kDPI"
   },
   "outputs": [],
   "source": [
    "class ELM(BaseEstimator, ClassifierMixin):\n",
    "         \n",
    "    def __init__(self, n_neurons):\n",
    "        self.n_neurons = n_neurons\n",
    "    \n",
    "    def fit(self, X, y, Z=[]):\n",
    "        X_new = np.ones((X.shape[0], X.shape[1]+1))\n",
    "        X_new[:,1:] = X\n",
    "        X = X_new\n",
    "        n = X.shape[1]\n",
    "        if len(Z) == 0:\n",
    "            self.Z = np.array([random.uniform(-1, 1) for i in range(n*self.n_neurons)]).reshape(n, self.n_neurons)\n",
    "        else:\n",
    "            self.Z = Z\n",
    "        self.H = sigmoid(np.dot(X, self.Z))\n",
    "        self.w = np.dot(np.linalg.pinv(self.H), y)  \n",
    "        return self.w, self.H, self.Z\n",
    "            \n",
    "    def predict(self, X):\n",
    "        X_new = np.ones((X.shape[0], X.shape[1]+1))\n",
    "        X_new[:,1:] = X\n",
    "        H = sigmoid(np.dot(X_new, self.Z))\n",
    "        y_predicted = np.sign(np.dot(H, self.w))\n",
    "        y_predicted[y_predicted==0]=1\n",
    "        return y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Capture Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_l(X, y, p):\n",
    "    # GridSearch for lambda and learning rate of IMA ELM\n",
    "    parameters = {'lambda_param':np.linspace(0.01, 10, 50)}\n",
    "    clf = ELM_IMA(n_neurons=p, delta_margin=10^-3, IMA_iterations=10, max_updates=10000)\n",
    "    clf = GridSearchCV(clf, parameters, scoring='accuracy', cv=5, verbose=0)\n",
    "    clf.fit(X, y)\n",
    "    return clf.best_params_['lambda_param']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(X, y, n_splits, p, eta, IMA_iterations, lambda_param):      \n",
    "    print(f'Parameters: p={p}, eta={eta}, lambda={lambda_param}')\n",
    "    # Stratified k fold cross validation\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=72)\n",
    "    i=0\n",
    "    \n",
    "    train_accuracy_IM_ELM_01 = np.zeros(n_splits)\n",
    "    test_accuracy_IM_ELM_01 = np.zeros(n_splits)\n",
    "    margin_IM_ELM_01 = np.zeros(n_splits)\n",
    "    updates_01 = np.zeros(n_splits)\n",
    "    iterations_FMP_01 = np.zeros(n_splits) \n",
    "    iterations_IMA_01 = np.zeros(n_splits)\n",
    "    hidden_neurons_01 = np.zeros(n_splits)\n",
    "    \n",
    "    train_accuracy_IM_ELM_001 = np.zeros(n_splits)\n",
    "    test_accuracy_IM_ELM_001 = np.zeros(n_splits)\n",
    "    margin_IM_ELM_001 = np.zeros(n_splits)\n",
    "    updates_001 = np.zeros(n_splits)\n",
    "    iterations_FMP_001 = np.zeros(n_splits) \n",
    "    iterations_IMA_001 = np.zeros(n_splits)\n",
    "    hidden_neurons_001 = np.zeros(n_splits)\n",
    "    \n",
    "    train_accuracy_IM_ELM_0001 = np.zeros(n_splits)\n",
    "    test_accuracy_IM_ELM_0001 = np.zeros(n_splits)\n",
    "    margin_IM_ELM_0001 = np.zeros(n_splits)\n",
    "    updates_0001 = np.zeros(n_splits)\n",
    "    iterations_FMP_0001 = np.zeros(n_splits) \n",
    "    iterations_IMA_0001 = np.zeros(n_splits)\n",
    "    hidden_neurons_0001 = np.zeros(n_splits)\n",
    "    \n",
    "    train_accuracy_ELM = np.zeros(n_splits)\n",
    "    test_accuracy_ELM = np.zeros(n_splits)\n",
    "    margin_ELM = np.zeros(n_splits)\n",
    "        \n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train = X[train_index,:]\n",
    "        X_test = X[test_index,:]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "               \n",
    "        # ELM\n",
    "        clf = ELM(n_neurons=p)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_hat=clf.predict(X_test)\n",
    "        y_hat_train=clf.predict(X_train)\n",
    "        margin_ELM[i] = compute_margin(clf.H[:,:], y_train, clf.w, 0)\n",
    "        train_accuracy_ELM[i] = accuracy_score(y_train, y_hat_train)   \n",
    "        test_accuracy_ELM[i] = accuracy_score(y_test, y_hat)\n",
    "        Z = clf.Z\n",
    "        \n",
    "        # ELM-IMA 0.1\n",
    "        clf_01 = ELM_IMA(n_neurons=p, eta=eta, lambda_param=lambda_param, delta_margin=10^-3,\n",
    "                      IMA_iterations=IMA_iterations, max_updates=10000, threshold=0.1)\n",
    "        iterations_IMA_01[i], updates_01[i], iterations_FMP_01[i] = clf_01.fit(X_train, y_train, Z)\n",
    "        y_hat=clf_01.predict(X_test, use_IMA_w = True)\n",
    "        y_hat_train=clf_01.predict(X_train, use_IMA_w = True)\n",
    "        margin_IM_ELM_01[i] = compute_margin(clf_01.H[:,:], y_train, clf_01.w, clf_01.b)\n",
    "        train_accuracy_IM_ELM_01[i] = accuracy_score(y_train, y_hat_train)   \n",
    "        test_accuracy_IM_ELM_01[i] = accuracy_score(y_test, y_hat)\n",
    "        hidden_neurons_01[i] = len(clf_01.w)\n",
    "        \n",
    "                \n",
    "        # ELM-IMA 0.01\n",
    "        clf_001 = ELM_IMA(n_neurons=p, eta=eta, lambda_param=lambda_param, delta_margin=10^-3,\n",
    "                      IMA_iterations=IMA_iterations, max_updates=10000, threshold=0.01)\n",
    "        iterations_IMA_001[i], updates_001[i], iterations_FMP_001[i] = clf_001.fit(X_train, y_train, Z)\n",
    "        y_hat=clf_001.predict(X_test, use_IMA_w = True)\n",
    "        y_hat_train=clf_001.predict(X_train, use_IMA_w = True)\n",
    "        margin_IM_ELM_001[i] = compute_margin(clf_001.H[:,:], y_train, clf_001.w, clf_001.b)\n",
    "        train_accuracy_IM_ELM_001[i] = accuracy_score(y_train, y_hat_train)   \n",
    "        test_accuracy_IM_ELM_001[i] = accuracy_score(y_test, y_hat)\n",
    "        hidden_neurons_001[i] = len(clf_001.w)\n",
    "\n",
    "        # ELM-IMA 0.001\n",
    "        clf_0001 = ELM_IMA(n_neurons=p, eta=eta, lambda_param=lambda_param, delta_margin=10^-3,\n",
    "                      IMA_iterations=IMA_iterations, max_updates=10000, threshold=0.001)\n",
    "        iterations_IMA_0001[i], updates_0001[i], iterations_FMP_0001[i] = clf_0001.fit(X_train, y_train, Z)\n",
    "        y_hat=clf_0001.predict(X_test, use_IMA_w = True)\n",
    "        y_hat_train=clf_0001.predict(X_train, use_IMA_w = True)\n",
    "        margin_IM_ELM_0001[i] = compute_margin(clf_0001.H[:,:], y_train, clf_0001.w, clf_0001.b)\n",
    "        train_accuracy_IM_ELM_0001[i] = accuracy_score(y_train, y_hat_train)   \n",
    "        test_accuracy_IM_ELM_0001[i] = accuracy_score(y_test, y_hat)\n",
    "        hidden_neurons_0001[i] = len(clf_0001.w)       \n",
    "        i+=1\n",
    "        \n",
    "    print(\"********* Results ELM-IMA 0.1**************\")\n",
    "    print(\"Acc train: \" + '{:.4f}'.format(train_accuracy_IM_ELM_01.mean())+ \"+/-\" + '{:.4f}'.format(train_accuracy_IM_ELM_01.std()))\n",
    "    print(\"Acc test: \" + '{:.4f}'.format(test_accuracy_IM_ELM_01.mean()) + \"+/-\" + '{:.4f}'.format(test_accuracy_IM_ELM_01.std()))\n",
    "    print(\"Iterations: \" + '{:.4f}'.format(iterations_FMP_01.mean())+ \"+/-\" + '{:.4f}'.format(iterations_FMP_01.std()))\n",
    "    print(\"Iterations IMA: \" + '{:.4f}'.format(iterations_IMA_01.mean())+ \"+/-\" + '{:.4f}'.format(iterations_IMA_01.std()))\n",
    "    print(\"Updates: \" + '{:.4f}'.format(updates_01.mean())+ \"+/-\" + '{:.4f}'.format(updates_01.std()))\n",
    "    print(\"Margin: \" + '{:.9f}'.format(margin_IM_ELM_01.mean())+ \"+/-\" + '{:.9f}'.format(margin_IM_ELM_01.std()))\n",
    "    print(\"Hidden Neurons: \" + '{:.2f}'.format(hidden_neurons_01.mean()) + \"+/-\" + '{:.2f}'.format(hidden_neurons_01.std()))\n",
    "    \n",
    "    print(\"********* Results ELM-IMA 0.01**************\")\n",
    "    print(\"Acc train: \" + '{:.4f}'.format(train_accuracy_IM_ELM_001.mean())+ \"+/-\" + '{:.4f}'.format(train_accuracy_IM_ELM_001.std()))\n",
    "    print(\"Acc test: \" + '{:.4f}'.format(test_accuracy_IM_ELM_001.mean()) + \"+/-\" + '{:.4f}'.format(test_accuracy_IM_ELM_001.std()))\n",
    "    print(\"Iterations: \" + '{:.4f}'.format(iterations_FMP_001.mean())+ \"+/-\" + '{:.4f}'.format(iterations_FMP_001.std()))\n",
    "    print(\"Iterations IMA: \" + '{:.4f}'.format(iterations_IMA_001.mean())+ \"+/-\" + '{:.4f}'.format(iterations_IMA_001.std()))\n",
    "    print(\"Updates: \" + '{:.4f}'.format(updates_001.mean())+ \"+/-\" + '{:.4f}'.format(updates_001.std()))\n",
    "    print(\"Margin: \" + '{:.9f}'.format(margin_IM_ELM_001.mean())+ \"+/-\" + '{:.9f}'.format(margin_IM_ELM_001.std()))\n",
    "    print(\"Hidden Neurons: \" + '{:.2f}'.format(hidden_neurons_001.mean()) + \"+/-\" + '{:.2f}'.format(hidden_neurons_001.std()))\n",
    "    \n",
    "    print(\"********* Results ELM-IMA 0.001**************\")\n",
    "    print(\"Acc train: \" + '{:.4f}'.format(train_accuracy_IM_ELM_0001.mean())+ \"+/-\" + '{:.4f}'.format(train_accuracy_IM_ELM_0001.std()))\n",
    "    print(\"Acc test: \" + '{:.4f}'.format(test_accuracy_IM_ELM_0001.mean()) + \"+/-\" + '{:.4f}'.format(test_accuracy_IM_ELM_0001.std()))\n",
    "    print(\"Iterations: \" + '{:.4f}'.format(iterations_FMP_0001.mean())+ \"+/-\" + '{:.4f}'.format(iterations_FMP_0001.std()))\n",
    "    print(\"Iterations IMA: \" + '{:.4f}'.format(iterations_IMA_0001.mean())+ \"+/-\" + '{:.4f}'.format(iterations_IMA_0001.std()))\n",
    "    print(\"Updates: \" + '{:.4f}'.format(updates_0001.mean())+ \"+/-\" + '{:.4f}'.format(updates_0001.std()))\n",
    "    print(\"Margin: \" + '{:.9f}'.format(margin_IM_ELM_0001.mean())+ \"+/-\" + '{:.9f}'.format(margin_IM_ELM_0001.std()))\n",
    "    print(\"Hidden Neurons: \" + '{:.2f}'.format(hidden_neurons_0001.mean()) + \"+/-\" + '{:.2f}'.format(hidden_neurons_0001.std()))\n",
    "            \n",
    "    print(\"********* Results ELM **************\")\n",
    "    print(\"Acc train: \" + '{:.4f}'.format(train_accuracy_ELM.mean())+ \"+/-\" + '{:.4f}'.format(train_accuracy_ELM.std()))\n",
    "    print(\"Acc test: \" + '{:.4f}'.format(test_accuracy_ELM.mean()) + \"+/-\" + '{:.4f}'.format(test_accuracy_ELM.std()))\n",
    "    print(\"Margin: \" + '{:.9f}'.format(margin_ELM.mean())+  \"+/-\" + '{:.9f}'.format(margin_ELM.std()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(X,y):\n",
    "    # Normalizing data:\n",
    "    normalizer = preprocessing.Normalizer()\n",
    "    X = normalizer.fit_transform(X)\n",
    "    n = len(X)\n",
    "    if n>1000:\n",
    "        n=1000\n",
    "    for p in [n]:\n",
    "        l = grid_l(X, y, p=p)\n",
    "        print(f\"Experimento com {p} neurônios:\" )\n",
    "        results(X, y, n_splits=10, p=p, eta=0.1, IMA_iterations=20, lambda_param=l)\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGsjlM93gUvW"
   },
   "source": [
    "## Application on Iris Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pRYAEinhgT0X",
    "outputId": "14dd5de2-37b1-49c2-ff7a-f8d254b84f22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 150 neurônios:\n",
      "Parameters: p=150, eta=0.1, lambda=0.01\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 1.0000+/-0.0000\n",
      "Iterations: 1232.9000+/-879.9007\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 3609.7000+/-1683.3828\n",
      "Margin: 0.116311370+/-0.017844497\n",
      "Hidden Neurons: 20.60+/-5.66\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 1.0000+/-0.0000\n",
      "Iterations: 1234.8000+/-539.9957\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 4547.4000+/-1149.3676\n",
      "Margin: 0.128129018+/-0.008188443\n",
      "Hidden Neurons: 27.40+/-9.24\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 1.0000+/-0.0000\n",
      "Iterations: 1341.9000+/-1018.4696\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 5492.4000+/-3222.2697\n",
      "Margin: 0.133863605+/-0.012691819\n",
      "Hidden Neurons: 79.40+/-21.48\n",
      "********* Results ELM **************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 1.0000+/-0.0000\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "# setosa - 0, versicolor - 1, virginica - 2  \n",
    "y = iris.target \n",
    "# O problema agora possui apenas as classes y=-1 e y=1\n",
    "y[y>0] = 1\n",
    "y[y==0] = -1\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_F6-t8hFB_q"
   },
   "source": [
    "## Application on Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Weyz8rxWFBIt",
    "outputId": "1ff193d4-c0ab-43c7-b90e-b1621657cdbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 600 neurônios:\n",
      "Parameters: p=600, eta=0.1, lambda=2.2526530612244895\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 0.9961+/-0.0063\n",
      "Acc test: 0.9867+/-0.0233\n",
      "Iterations: 201.8000+/-112.9733\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 3730.5000+/-2037.2679\n",
      "Margin: 0.001476860+/-0.002655616\n",
      "Hidden Neurons: 345.40+/-54.62\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 0.9952+/-0.0044\n",
      "Acc test: 0.9750+/-0.0403\n",
      "Iterations: 176.7000+/-65.9273\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 3425.0000+/-1651.8901\n",
      "Margin: 0.002834857+/-0.004935276\n",
      "Hidden Neurons: 498.80+/-22.63\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 0.9972+/-0.0030\n",
      "Acc test: 0.9850+/-0.0345\n",
      "Iterations: 216.8000+/-111.5480\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 5062.5000+/-3864.4110\n",
      "Margin: 0.000829275+/-0.001304060\n",
      "Hidden Neurons: 549.50+/-56.61\n",
      "********* Results ELM **************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 0.7067+/-0.0638\n",
      "Margin: 0.000131400+/-0.000012691\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "synthetic_dataset = pd.read_csv('data/synthetic_dataset/synthetic_control.data', sep=\"\\s+\",  header=None, engine='python')\n",
    "X = synthetic_dataset.to_numpy()\n",
    "y = np.concatenate((np.ones(100), np.ones(200)*-1, np.ones(100), np.ones(100)*-1,np.ones(100)))\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEyDIiTZoGap"
   },
   "source": [
    "## Application on Robot  Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUaMED7moS6p",
    "outputId": "1af62482-971b-4204-cd1e-b813d963551a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 117 neurônios:\n",
      "Parameters: p=117, eta=0.1, lambda=8.572857142857142\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 0.9915+/-0.0067\n",
      "Acc test: 0.9833+/-0.0500\n",
      "Iterations: 79.1000+/-17.7620\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 237.9000+/-73.8816\n",
      "Margin: 0.012204205+/-0.023850406\n",
      "Hidden Neurons: 44.10+/-13.88\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 0.9962+/-0.0047\n",
      "Acc test: 0.9750+/-0.0534\n",
      "Iterations: 76.4000+/-16.7404\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 246.3000+/-78.5825\n",
      "Margin: 0.049671701+/-0.058164843\n",
      "Hidden Neurons: 71.90+/-13.34\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 0.9924+/-0.0093\n",
      "Acc test: 0.9917+/-0.0250\n",
      "Iterations: 86.4000+/-30.8843\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 327.1000+/-162.8695\n",
      "Margin: 0.026076210+/-0.031512028\n",
      "Hidden Neurons: 100.20+/-8.49\n",
      "********* Results ELM **************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 0.8432+/-0.1589\n",
      "Margin: 0.016406460+/-0.003271136\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "robot_dataset = pd.read_csv('data/robot/lp4_data.csv', delimiter =',')\n",
    "X = robot_dataset.to_numpy().reshape([117,90])\n",
    "y = np.concatenate((np.ones(24), np.ones(117-24)*-1))\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on Mushroom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 1000 neurônios:\n",
      "Parameters: p=1000, eta=0.1, lambda=8.572857142857142\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 0.9816+/-0.0553\n",
      "Iterations: 83.4000+/-33.1488\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 1550.9000+/-453.4778\n",
      "Margin: 0.161095918+/-0.046215993\n",
      "Hidden Neurons: 512.90+/-103.67\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 0.9816+/-0.0553\n",
      "Iterations: 87.5000+/-39.3961\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 1712.0000+/-859.5726\n",
      "Margin: 0.142506384+/-0.059292175\n",
      "Hidden Neurons: 781.70+/-119.52\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 0.9816+/-0.0553\n",
      "Iterations: 84.3000+/-40.3387\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 1769.1000+/-1181.3681\n",
      "Margin: 0.130323204+/-0.057381014\n",
      "Hidden Neurons: 894.90+/-122.84\n",
      "********* Results ELM **************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 0.9821+/-0.0537\n",
      "Margin: 0.124361135+/-0.011935599\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/Mushroom/agaricus-lepiota.data', delimiter =',', header=None)\n",
    "df = df.replace(\"?\", np.nan) \n",
    "df = df.dropna() \n",
    "y = df[0].to_numpy()\n",
    "X = df.drop([0], axis='columns')\n",
    "X = pd.get_dummies(X).to_numpy()\n",
    "y[np.where(y=='e')] = -1\n",
    "y[np.where(y=='p')] = 1\n",
    "y = np.array(y.tolist())\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on Ionosphere Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 351 neurônios:\n",
      "Parameters: p=351, eta=0.1, lambda=7.349591836734694\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 0.9452+/-0.0100\n",
      "Acc test: 0.9117+/-0.0449\n",
      "Iterations: 212.9000+/-51.8969\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 2576.8000+/-254.5835\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 135.90+/-28.44\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 0.9490+/-0.0139\n",
      "Acc test: 0.9145+/-0.0495\n",
      "Iterations: 223.8000+/-87.8679\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 3011.5000+/-775.5004\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 238.80+/-35.85\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 0.9405+/-0.0230\n",
      "Acc test: 0.8918+/-0.0521\n",
      "Iterations: 247.3000+/-109.1055\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 3626.0000+/-1071.1322\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 315.40+/-21.50\n",
      "********* Results ELM **************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 0.7467+/-0.0828\n",
      "Margin: 0.000801092+/-0.000294789\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ionosphere_dataset = pd.read_csv('data/Ionosphere/ionosphere.data', names=list(range(0,35)), sep=',')\n",
    "y = ionosphere_dataset[34].to_numpy()\n",
    "X = ionosphere_dataset.drop([34], axis='columns').to_numpy()\n",
    "y[np.where(y=='g')] = 1\n",
    "y[np.where(y=='b')] = -1\n",
    "y = np.array(y.tolist())\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on Banknote Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 1000 neurônios:\n",
      "Parameters: p=1000, eta=0.1, lambda=0.01\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 0.9550+/-0.1332\n",
      "Acc test: 0.9533+/-0.1329\n",
      "Iterations: 2244.6000+/-951.7887\n",
      "Iterations IMA: 16.0000+/-7.3075\n",
      "Updates: 20498.3000+/-7920.7612\n",
      "Margin: 0.028867533+/-0.018952747\n",
      "Hidden Neurons: 477.10+/-229.11\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 0.9555+/-0.1334\n",
      "Acc test: 0.9540+/-0.1331\n",
      "Iterations: 1955.9000+/-1202.2978\n",
      "Iterations IMA: 16.9000+/-6.3945\n",
      "Updates: 18799.7000+/-7779.4331\n",
      "Margin: 0.044073148+/-0.016603311\n",
      "Hidden Neurons: 747.50+/-129.17\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 0.9552+/-0.1333\n",
      "Acc test: 0.9533+/-0.1329\n",
      "Iterations: 1861.8000+/-866.9453\n",
      "Iterations IMA: 14.7000+/-7.4169\n",
      "Updates: 19662.8000+/-6632.1591\n",
      "Margin: 0.037132139+/-0.020741476\n",
      "Hidden Neurons: 880.80+/-85.84\n",
      "********* Results ELM **************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 0.9322+/-0.0176\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in banknote authentication set\n",
    "banknotes = pd.read_csv('data/banknote/data_banknote_authentication.txt', names=['variance', 'skewness', 'curtosis', 'entropy', 'class'])\n",
    "# convert to array\n",
    "X = banknotes[['variance', 'skewness', 'curtosis', 'entropy']].to_numpy()\n",
    "y = banknotes[['class']].to_numpy()\n",
    "y[np.where(y==0)] = -1\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 178 neurônios:\n",
      "Parameters: p=178, eta=0.1, lambda=0.8255102040816327\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 0.6916+/-0.0427\n",
      "Acc test: 0.7137+/-0.1474\n",
      "Iterations: 568.5000+/-131.5236\n",
      "Iterations IMA: 18.4000+/-4.8000\n",
      "Updates: 13164.0000+/-2863.1405\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 53.90+/-25.64\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 0.7029+/-0.0568\n",
      "Acc test: 0.7095+/-0.1368\n",
      "Iterations: 619.0000+/-222.9910\n",
      "Iterations IMA: 18.4000+/-4.8000\n",
      "Updates: 15635.3000+/-5066.6394\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 89.30+/-34.58\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 0.6928+/-0.1073\n",
      "Acc test: 0.6987+/-0.1917\n",
      "Iterations: 688.1000+/-238.9081\n",
      "Iterations IMA: 18.4000+/-4.8000\n",
      "Updates: 20009.7000+/-6253.8259\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 119.90+/-33.16\n",
      "********* Results ELM **************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 0.6748+/-0.1177\n",
      "Margin: 0.000000001+/-0.000000000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wine_dataset = pd.read_csv('data/wine/wine.data', names=['Class', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315', 'Proline'])\n",
    "# convert to array\n",
    "y = wine_dataset[['Class']].to_numpy()\n",
    "X = wine_dataset.drop(\"Class\",axis='columns').to_numpy()\n",
    "y[np.where(y==3)] = 1\n",
    "y[np.where(y==2)] = -1\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on WDBC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 569 neurônios:\n",
      "Parameters: p=569, eta=0.1, lambda=7.75734693877551\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 0.9012+/-0.0192\n",
      "Acc test: 0.8981+/-0.0449\n",
      "Iterations: 319.3000+/-250.7720\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 7171.4000+/-3627.0806\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 312.90+/-56.40\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 0.8834+/-0.0380\n",
      "Acc test: 0.8788+/-0.0539\n",
      "Iterations: 403.4000+/-395.2175\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 9620.4000+/-4753.6439\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 424.80+/-62.61\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 0.8819+/-0.0435\n",
      "Acc test: 0.8717+/-0.0509\n",
      "Iterations: 600.1000+/-472.9377\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 12070.6000+/-6093.5300\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 494.20+/-56.73\n",
      "********* Results ELM **************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 0.7083+/-0.0836\n",
      "Margin: 0.000000001+/-0.000000000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wdbc_dataset = pd.read_csv('data/WDBC/wdbc.data', names=list(range(0,32)))\n",
    "# convert to array\n",
    "y = wdbc_dataset[1].to_numpy()\n",
    "X = wdbc_dataset.drop([0, 1],axis='columns').to_numpy()\n",
    "y[np.where(y=='B')] = 1\n",
    "y[np.where(y=='M')] = -1\n",
    "y = np.array(y.tolist())\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on Sonar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 208 neurônios:\n",
      "Parameters: p=208, eta=0.1, lambda=5.514693877551021\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 0.7606+/-0.0735\n",
      "Acc test: 0.6207+/-0.1455\n",
      "Iterations: 237.5000+/-74.4302\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 4192.3000+/-811.7347\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 73.10+/-22.96\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 0.7821+/-0.0728\n",
      "Acc test: 0.6588+/-0.1154\n",
      "Iterations: 281.0000+/-110.6689\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 5474.1000+/-1427.1387\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 116.20+/-35.13\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 0.7816+/-0.0712\n",
      "Acc test: 0.6205+/-0.1584\n",
      "Iterations: 283.2000+/-73.2350\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 5933.7000+/-1677.3815\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 175.70+/-17.10\n",
      "********* Results ELM **************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 0.5874+/-0.1322\n",
      "Margin: 0.000522685+/-0.000110384\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sonar_dataset = pd.read_csv('data/sonar/sonar.all-data', names=list(range(0,61)), sep=',')\n",
    "y = sonar_dataset[60].to_numpy()\n",
    "X = sonar_dataset.drop([60], axis='columns').to_numpy()\n",
    "y[np.where(y=='R')] = 1\n",
    "y[np.where(y=='M')] = -1\n",
    "y = np.array(y.tolist())\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on Pima Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 768 neurônios:\n",
      "Parameters: p=768, eta=0.1, lambda=2.0487755102040817\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 0.6549+/-0.0093\n",
      "Acc test: 0.6510+/-0.0193\n",
      "Iterations: 107.8000+/-148.3265\n",
      "Iterations IMA: 1.5000+/-0.6708\n",
      "Updates: 13332.6000+/-5485.2641\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 680.10+/-115.61\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 0.6515+/-0.0032\n",
      "Acc test: 0.6523+/-0.0196\n",
      "Iterations: 43.4000+/-18.8478\n",
      "Iterations IMA: 1.4000+/-0.4899\n",
      "Updates: 12402.0000+/-3023.3864\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 762.40+/-7.13\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 0.6513+/-0.0030\n",
      "Acc test: 0.6523+/-0.0196\n",
      "Iterations: 43.3000+/-18.7406\n",
      "Iterations IMA: 1.4000+/-0.4899\n",
      "Updates: 12379.8000+/-2999.4686\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 767.30+/-1.00\n",
      "********* Results ELM **************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 0.4857+/-0.0491\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pima_dataset = pd.read_csv('data/diabetes/diabetes.csv', sep=\",\", engine='python')\n",
    "y = pima_dataset['Outcome'].to_numpy()\n",
    "X = pima_dataset.drop(['Outcome'], axis='columns').to_numpy()\n",
    "y[np.where(y==0)] = -1\n",
    "y = np.array(y.tolist())\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on Statlog Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 270 neurônios:\n",
      "Parameters: p=270, eta=0.1, lambda=0.6216326530612245\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 0.7000+/-0.0392\n",
      "Acc test: 0.6963+/-0.1018\n",
      "Iterations: 606.3000+/-184.9768\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 31485.4000+/-11615.5041\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 57.30+/-21.54\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 0.6827+/-0.0894\n",
      "Acc test: 0.6778+/-0.1273\n",
      "Iterations: 683.9000+/-430.4780\n",
      "Iterations IMA: 18.8000+/-3.6000\n",
      "Updates: 42281.8000+/-28400.8517\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 156.60+/-44.01\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 0.6329+/-0.0915\n",
      "Acc test: 0.6519+/-0.1138\n",
      "Iterations: 582.6000+/-406.9688\n",
      "Iterations IMA: 14.4000+/-7.0456\n",
      "Updates: 36303.4000+/-24429.7366\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 233.70+/-33.64\n",
      "********* Results ELM **************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 0.5889+/-0.1400\n",
      "Margin: 0.000000016+/-0.000000003\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statlog_dataset = pd.read_csv('data/statlog/heart.dat', sep=\" \", header=None, engine='python')\n",
    "y = statlog_dataset[13].to_numpy()\n",
    "X = statlog_dataset.drop([13], axis='columns').to_numpy()\n",
    "y[np.where(y==2)] = -1\n",
    "y = np.array(y.tolist())\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on Mammographic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 830 neurônios:\n",
      "Parameters: p=830, eta=0.1, lambda=7.75734693877551\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 0.6075+/-0.0814\n",
      "Acc test: 0.6000+/-0.0890\n",
      "Iterations: 614.3000+/-425.3975\n",
      "Iterations IMA: 18.1000+/-5.7000\n",
      "Updates: 38327.4000+/-15368.3756\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 343.70+/-189.68\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 0.6050+/-0.0901\n",
      "Acc test: 0.6133+/-0.1170\n",
      "Iterations: 588.5000+/-438.0580\n",
      "Iterations IMA: 15.7000+/-7.0434\n",
      "Updates: 52686.8000+/-26593.0050\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 483.40+/-199.58\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 0.5973+/-0.0908\n",
      "Acc test: 0.5904+/-0.1002\n",
      "Iterations: 394.5000+/-273.4086\n",
      "Iterations IMA: 15.0000+/-7.7201\n",
      "Updates: 44940.0000+/-27767.1665\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 699.40+/-69.60\n",
      "********* Results ELM **************\n",
      "Acc train: 0.8869+/-0.0051\n",
      "Acc test: 0.7614+/-0.0420\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mammo = pd.read_csv('data/mammographic/mammographic_masses.data', sep=\",\", header=None, engine='python')\n",
    "mammo = mammo.replace(\"?\", np.nan)\n",
    "mammo = mammo.dropna()\n",
    "y = mammo[5].to_numpy()\n",
    "X = mammo.drop([5], axis='columns').to_numpy()\n",
    "y[np.where(y==0)] = -1\n",
    "y = np.array(y.tolist())\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on Haberman Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 306 neurônios:\n",
      "Parameters: p=306, eta=0.1, lambda=0.8255102040816327\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 0.7051+/-0.1045\n",
      "Acc test: 0.7290+/-0.0488\n",
      "Iterations: 310.0000+/-124.9328\n",
      "Iterations IMA: 4.8000+/-3.4583\n",
      "Updates: 20784.5000+/-2495.6583\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 46.70+/-28.17\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 0.7357+/-0.0021\n",
      "Acc test: 0.7353+/-0.0094\n",
      "Iterations: 363.3000+/-330.9514\n",
      "Iterations IMA: 5.2000+/-4.3543\n",
      "Updates: 24751.2000+/-9786.5257\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 182.40+/-59.83\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 0.7357+/-0.0016\n",
      "Acc test: 0.7353+/-0.0094\n",
      "Iterations: 261.4000+/-81.7829\n",
      "Iterations IMA: 3.8000+/-1.2490\n",
      "Updates: 21789.1000+/-4561.9412\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 270.10+/-30.22\n",
      "********* Results ELM **************\n",
      "Acc train: 0.8315+/-0.0069\n",
      "Acc test: 0.7286+/-0.0373\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "haberman = pd.read_csv('data/haberman/haberman.data', sep=\",\", header=None, engine='python')\n",
    "y = haberman[3].to_numpy()\n",
    "X = haberman.drop([3], axis='columns').to_numpy()\n",
    "y[np.where(y==2)] = -1\n",
    "y = np.array(y.tolist())\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on Transfusion Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 748 neurônios:\n",
      "Parameters: p=748, eta=0.1, lambda=0.41775510204081634\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 0.7044+/-0.0917\n",
      "Acc test: 0.7021+/-0.1594\n",
      "Iterations: 129.2000+/-116.3450\n",
      "Iterations IMA: 1.3000+/-0.4583\n",
      "Updates: 10451.9000+/-484.4080\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 708.90+/-59.73\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 0.7430+/-0.0359\n",
      "Acc test: 0.7034+/-0.1589\n",
      "Iterations: 127.9000+/-116.0771\n",
      "Iterations IMA: 1.3000+/-0.4583\n",
      "Updates: 10463.1000+/-484.1238\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 744.40+/-6.17\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 0.7436+/-0.0339\n",
      "Acc test: 0.7034+/-0.1589\n",
      "Iterations: 127.9000+/-116.0771\n",
      "Iterations IMA: 1.3000+/-0.4583\n",
      "Updates: 10464.9000+/-487.3183\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 747.70+/-0.46\n",
      "********* Results ELM **************\n",
      "Acc train: 0.7947+/-0.0112\n",
      "Acc test: 0.7327+/-0.1153\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transfusion = pd.read_csv('data/transfusion/transfusion.data', sep=\",\", engine='python')\n",
    "y = transfusion[\"whether he/she donated blood in March 2007\"].to_numpy()\n",
    "X = transfusion.drop([\"whether he/she donated blood in March 2007\"], axis='columns').to_numpy()\n",
    "y[np.where(y==0)] = -1\n",
    "y = np.array(y.tolist())\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on Australian Credit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 690 neurônios:\n",
      "Parameters: p=690, eta=0.1, lambda=7.961224489795918\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 0.7097+/-0.0483\n",
      "Acc test: 0.7261+/-0.0603\n",
      "Iterations: 523.2000+/-146.0327\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 30883.5000+/-6761.8284\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 252.40+/-38.50\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 0.7403+/-0.0348\n",
      "Acc test: 0.7536+/-0.0506\n",
      "Iterations: 492.9000+/-162.4078\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 33415.4000+/-6299.7893\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 460.20+/-74.60\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 0.7501+/-0.0162\n",
      "Acc test: 0.7580+/-0.0523\n",
      "Iterations: 508.7000+/-148.7293\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 38597.2000+/-10497.8891\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 647.90+/-11.34\n",
      "********* Results ELM **************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 0.5623+/-0.0691\n",
      "Margin: 0.000000002+/-0.000000000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "australian = pd.read_csv('data/australian_credit/australian.dat', header=None, sep=\" \", engine='python')\n",
    "australian = australian.replace(\"?\", np.nan)\n",
    "australian = australian.dropna()\n",
    "y = australian[14].to_numpy()\n",
    "X = australian.drop([14], axis='columns').to_numpy()\n",
    "y[np.where(y==0)] = -1\n",
    "y = np.array(y.tolist())\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 683 neurônios:\n",
      "Parameters: p=683, eta=0.1, lambda=2.660408163265306\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 0.8640+/-0.0208\n",
      "Acc test: 0.8332+/-0.0651\n",
      "Iterations: 377.4000+/-70.8099\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 22727.3000+/-2813.0048\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 48.10+/-28.88\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 0.8388+/-0.0343\n",
      "Acc test: 0.8289+/-0.0632\n",
      "Iterations: 649.3000+/-202.3384\n",
      "Iterations IMA: 18.2000+/-5.4000\n",
      "Updates: 42812.5000+/-12703.4053\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 151.10+/-178.74\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 0.8206+/-0.0331\n",
      "Acc test: 0.8142+/-0.0593\n",
      "Iterations: 1306.9000+/-179.2738\n",
      "Iterations IMA: 20.0000+/-0.0000\n",
      "Updates: 93392.1000+/-15024.8907\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 332.60+/-74.11\n",
      "********* Results ELM **************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 0.7851+/-0.0602\n",
      "Margin: 0.000010104+/-0.000000816\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "breast = pd.read_csv('data/breast/breast.data', header=None, sep=\",\", engine='python')\n",
    "breast = breast.replace(\"?\", np.nan)\n",
    "breast = breast.dropna()\n",
    "y = breast[10].to_numpy()\n",
    "X = breast.drop([0, 10], axis='columns').to_numpy()\n",
    "y[np.where(y==4)] = 1\n",
    "y[np.where(y==2)] = -1\n",
    "y = np.array(y.tolist())\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on Spam Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 1000 neurônios:\n",
      "Parameters: p=1000, eta=0.1, lambda=8.572857142857142\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 0.6060+/-0.0001\n",
      "Acc test: 0.6060+/-0.0009\n",
      "Iterations: 161.7000+/-129.8908\n",
      "Iterations IMA: 1.0000+/-0.0000\n",
      "Updates: 10240.9000+/-222.9293\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 1000.00+/-0.00\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 0.6060+/-0.0001\n",
      "Acc test: 0.6060+/-0.0009\n",
      "Iterations: 161.7000+/-129.8908\n",
      "Iterations IMA: 1.0000+/-0.0000\n",
      "Updates: 10240.9000+/-222.9293\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 1000.00+/-0.00\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 0.6060+/-0.0001\n",
      "Acc test: 0.6060+/-0.0009\n",
      "Iterations: 161.7000+/-129.8908\n",
      "Iterations IMA: 1.0000+/-0.0000\n",
      "Updates: 10240.9000+/-222.9293\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 1000.00+/-0.00\n",
      "********* Results ELM **************\n",
      "Acc train: 0.9283+/-0.0042\n",
      "Acc test: 0.8229+/-0.0493\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam = pd.read_csv('data/spam/spambase.data', header=None, sep=\",\", engine='python')\n",
    "y = spam[57].to_numpy()\n",
    "X = spam.drop([57], axis='columns').to_numpy()\n",
    "y[np.where(y==0)] = -1\n",
    "y = np.array(y.tolist())\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on Glass Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento com 214 neurônios:\n",
      "Parameters: p=214, eta=0.1, lambda=0.01\n",
      "********* Results ELM-IMA 0.1**************\n",
      "Acc train: 0.6630+/-0.0300\n",
      "Acc test: 0.6364+/-0.1063\n",
      "Iterations: 118.8000+/-34.8362\n",
      "Iterations IMA: 1.2000+/-0.6000\n",
      "Updates: 10561.4000+/-1462.7818\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 207.30+/-20.10\n",
      "********* Results ELM-IMA 0.01**************\n",
      "Acc train: 0.6719+/-0.0035\n",
      "Acc test: 0.6773+/-0.0180\n",
      "Iterations: 114.8000+/-23.2370\n",
      "Iterations IMA: 1.1000+/-0.3000\n",
      "Updates: 10326.4000+/-758.0130\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 213.60+/-1.20\n",
      "********* Results ELM-IMA 0.001**************\n",
      "Acc train: 0.6719+/-0.0035\n",
      "Acc test: 0.6773+/-0.0180\n",
      "Iterations: 114.8000+/-23.2370\n",
      "Iterations IMA: 1.1000+/-0.3000\n",
      "Updates: 10320.2000+/-739.4250\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "Hidden Neurons: 214.00+/-0.00\n",
      "********* Results ELM **************\n",
      "Acc train: 1.0000+/-0.0000\n",
      "Acc test: 0.6165+/-0.1249\n",
      "Margin: 0.000000000+/-0.000000000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset:\n",
    "headers = [\"Id\", \"RI\", \"Na\", \"Mg\", \"Al\", \"Si\", \"K\", \"Ca\", \"Ba\", \"Fe\", \"Class\"]\n",
    "df = pd.read_csv(\"~/Documents/UFMG/Graduation/10/Reconhecimento de padrões/list/pattern-recognition-exercises/list_5/databases/glass.csv\", names = headers)\n",
    "X = df.drop(\"Class\", axis=1)\n",
    "X = X.drop(\"Id\", axis=1)\n",
    "y = df[\"Class\"]\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "y[np.where(y>1)] = -1\n",
    "run(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2    3    4    5    6    7    8    9   10   11   12   13\n",
      "   14   15   16   17   18   19   20   21   22   23   24   25   26   27\n",
      "   28   29   30   31   32   33   34   35   36   37   38   39   40   41\n",
      "   42   43   44   45   46   47   48   49   50   51   52   53   54   55\n",
      "   56   57   58   59   60   61   62   63   64   65   66   67   68   69\n",
      "   70   71   72   73   74   75   76   77   78   79   80   81   82   83\n",
      "   84   85   86   87   88   89   90   91   92   93   94   95   96   97\n",
      "   98   99  100  101  102  103  104  105  106  107  108  109  110  111\n",
      "  112  113  114  115  116  117  118  119  120  121  122  123  124  125\n",
      "  126  127  128  129  130  131  132  133  134  135  136  137  138  139\n",
      "  140  141  142  143  144  145  146  147  148  149  150  151  152  153\n",
      "  154  155  156  157  158  159  160  161  162  163  164  165  166  167\n",
      "  168  169  170  171  172  173  174  175  176  177  178  179  180  181\n",
      " 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826\n",
      " 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840\n",
      " 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854\n",
      " 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868\n",
      " 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882\n",
      " 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896\n",
      " 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910\n",
      " 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924\n",
      " 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938\n",
      " 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952\n",
      " 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966\n",
      " 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980\n",
      " 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994\n",
      " 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008\n",
      " 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022\n",
      " 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036\n",
      " 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050\n",
      " 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064\n",
      " 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078\n",
      " 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091]\n",
      "[ 182  183  184  185  186  187  188  189  190  191  192  193  194  195\n",
      "  196  197  198  199  200  201  202  203  204  205  206  207  208  209\n",
      "  210  211  212  213  214  215  216  217  218  219  220  221  222  223\n",
      "  224  225  226  227  228  229  230  231  232  233  234  235  236  237\n",
      "  238  239  240  241  242  243  244  245  246  247  248  249  250  251\n",
      "  252  253  254  255  256  257  258  259  260  261  262  263  264  265\n",
      "  266  267  268  269  270  271  272  273  274  275  276  277  278  279\n",
      "  280  281  282  283  284  285  286  287  288  289  290  291  292  293\n",
      "  294  295  296  297  298  299  300  301  302  303  304  305  306  307\n",
      "  308  309  310  311  312  313  314  315  316  317  318  319  320  321\n",
      "  322  323  324  325  326  327  328  329  330  331  332  333  334  335\n",
      "  336  337  338  339  340  341  342  343  344  345  346  347  348  349\n",
      "  350  351  352  353  354  355  356  357  358  359  360  361  362  363\n",
      " 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105\n",
      " 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119\n",
      " 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133\n",
      " 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147\n",
      " 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161\n",
      " 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175\n",
      " 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189\n",
      " 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203\n",
      " 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217\n",
      " 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231\n",
      " 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245\n",
      " 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259\n",
      " 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273\n",
      " 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287\n",
      " 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301\n",
      " 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315\n",
      " 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329\n",
      " 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343\n",
      " 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357\n",
      " 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369]\n",
      "[ 364  365  366  367  368  369  370  371  372  373  374  375  376  377\n",
      "  378  379  380  381  382  383  384  385  386  387  388  389  390  391\n",
      "  392  393  394  395  396  397  398  399  400  401  402  403  404  405\n",
      "  406  407  408  409  410  411  412  413  414  415  416  417  418  419\n",
      "  420  421  422  423  424  425  426  427  428  429  430  431  432  433\n",
      "  434  435  436  437  438  439  440  441  442  443  444  445  446  447\n",
      "  448  449  450  451  452  453  454  455  456  457  458  459  460  461\n",
      "  462  463  464  465  466  467  468  469  470  471  472  473  474  475\n",
      "  476  477  478  479  480  481  482  483  484  485  486  487  488  489\n",
      "  490  491  492  493  494  495  496  497  498  499  500  501  502  503\n",
      "  504  505  506  507  508  509  510  511  512  513  514  515  516  517\n",
      "  518  519  520  521  522  523  524  525  526  527  528  529  530  531\n",
      "  532  533  534  535  536  537  538  539  540  541  542  543  544  545\n",
      " 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383\n",
      " 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397\n",
      " 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411\n",
      " 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425\n",
      " 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439\n",
      " 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453\n",
      " 2454 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467\n",
      " 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481\n",
      " 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495\n",
      " 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509\n",
      " 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520 2521 2522 2523\n",
      " 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537\n",
      " 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551\n",
      " 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565\n",
      " 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577 2578 2579\n",
      " 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591 2592 2593\n",
      " 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607\n",
      " 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621\n",
      " 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635\n",
      " 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645 2646 2647]\n",
      "[ 546  547  548  549  550  551  552  553  554  555  556  557  558  559\n",
      "  560  561  562  563  564  565  566  567  568  569  570  571  572  573\n",
      "  574  575  576  577  578  579  580  581  582  583  584  585  586  587\n",
      "  588  589  590  591  592  593  594  595  596  597  598  599  600  601\n",
      "  602  603  604  605  606  607  608  609  610  611  612  613  614  615\n",
      "  616  617  618  619  620  621  622  623  624  625  626  627  628  629\n",
      "  630  631  632  633  634  635  636  637  638  639  640  641  642  643\n",
      "  644  645  646  647  648  649  650  651  652  653  654  655  656  657\n",
      "  658  659  660  661  662  663  664  665  666  667  668  669  670  671\n",
      "  672  673  674  675  676  677  678  679  680  681  682  683  684  685\n",
      "  686  687  688  689  690  691  692  693  694  695  696  697  698  699\n",
      "  700  701  702  703  704  705  706  707  708  709  710  711  712  713\n",
      "  714  715  716  717  718  719  720  721  722  723  724  725  726 2648\n",
      " 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662\n",
      " 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676\n",
      " 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690\n",
      " 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704\n",
      " 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718\n",
      " 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728 2729 2730 2731 2732\n",
      " 2733 2734 2735 2736 2737 2738 2739 2740 2741 2742 2743 2744 2745 2746\n",
      " 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757 2758 2759 2760\n",
      " 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771 2772 2773 2774\n",
      " 2775 2776 2777 2778 2779 2780 2781 2782 2783 2784 2785 2786 2787 2788\n",
      " 2789 2790 2791 2792 2793 2794 2795 2796 2797 2798 2799 2800 2801 2802\n",
      " 2803 2804 2805 2806 2807 2808 2809 2810 2811 2812 2813 2814 2815 2816\n",
      " 2817 2818 2819 2820 2821 2822 2823 2824 2825 2826 2827 2828 2829 2830\n",
      " 2831 2832 2833 2834 2835 2836 2837 2838 2839 2840 2841 2842 2843 2844\n",
      " 2845 2846 2847 2848 2849 2850 2851 2852 2853 2854 2855 2856 2857 2858\n",
      " 2859 2860 2861 2862 2863 2864 2865 2866 2867 2868 2869 2870 2871 2872\n",
      " 2873 2874 2875 2876 2877 2878 2879 2880 2881 2882 2883 2884 2885 2886\n",
      " 2887 2888 2889 2890 2891 2892 2893 2894 2895 2896 2897 2898 2899 2900\n",
      " 2901 2902 2903 2904 2905 2906 2907 2908 2909 2910 2911 2912 2913 2914\n",
      " 2915 2916 2917 2918 2919 2920 2921 2922 2923 2924 2925 2926]\n",
      "[ 727  728  729  730  731  732  733  734  735  736  737  738  739  740\n",
      "  741  742  743  744  745  746  747  748  749  750  751  752  753  754\n",
      "  755  756  757  758  759  760  761  762  763  764  765  766  767  768\n",
      "  769  770  771  772  773  774  775  776  777  778  779  780  781  782\n",
      "  783  784  785  786  787  788  789  790  791  792  793  794  795  796\n",
      "  797  798  799  800  801  802  803  804  805  806  807  808  809  810\n",
      "  811  812  813  814  815  816  817  818  819  820  821  822  823  824\n",
      "  825  826  827  828  829  830  831  832  833  834  835  836  837  838\n",
      "  839  840  841  842  843  844  845  846  847  848  849  850  851  852\n",
      "  853  854  855  856  857  858  859  860  861  862  863  864  865  866\n",
      "  867  868  869  870  871  872  873  874  875  876  877  878  879  880\n",
      "  881  882  883  884  885  886  887  888  889  890  891  892  893  894\n",
      "  895  896  897  898  899  900  901  902  903  904  905  906  907 2927\n",
      " 2928 2929 2930 2931 2932 2933 2934 2935 2936 2937 2938 2939 2940 2941\n",
      " 2942 2943 2944 2945 2946 2947 2948 2949 2950 2951 2952 2953 2954 2955\n",
      " 2956 2957 2958 2959 2960 2961 2962 2963 2964 2965 2966 2967 2968 2969\n",
      " 2970 2971 2972 2973 2974 2975 2976 2977 2978 2979 2980 2981 2982 2983\n",
      " 2984 2985 2986 2987 2988 2989 2990 2991 2992 2993 2994 2995 2996 2997\n",
      " 2998 2999 3000 3001 3002 3003 3004 3005 3006 3007 3008 3009 3010 3011\n",
      " 3012 3013 3014 3015 3016 3017 3018 3019 3020 3021 3022 3023 3024 3025\n",
      " 3026 3027 3028 3029 3030 3031 3032 3033 3034 3035 3036 3037 3038 3039\n",
      " 3040 3041 3042 3043 3044 3045 3046 3047 3048 3049 3050 3051 3052 3053\n",
      " 3054 3055 3056 3057 3058 3059 3060 3061 3062 3063 3064 3065 3066 3067\n",
      " 3068 3069 3070 3071 3072 3073 3074 3075 3076 3077 3078 3079 3080 3081\n",
      " 3082 3083 3084 3085 3086 3087 3088 3089 3090 3091 3092 3093 3094 3095\n",
      " 3096 3097 3098 3099 3100 3101 3102 3103 3104 3105 3106 3107 3108 3109\n",
      " 3110 3111 3112 3113 3114 3115 3116 3117 3118 3119 3120 3121 3122 3123\n",
      " 3124 3125 3126 3127 3128 3129 3130 3131 3132 3133 3134 3135 3136 3137\n",
      " 3138 3139 3140 3141 3142 3143 3144 3145 3146 3147 3148 3149 3150 3151\n",
      " 3152 3153 3154 3155 3156 3157 3158 3159 3160 3161 3162 3163 3164 3165\n",
      " 3166 3167 3168 3169 3170 3171 3172 3173 3174 3175 3176 3177 3178 3179\n",
      " 3180 3181 3182 3183 3184 3185 3186 3187 3188 3189 3190 3191 3192 3193\n",
      " 3194 3195 3196 3197 3198 3199 3200 3201 3202 3203 3204 3205]\n",
      "[ 908  909  910  911  912  913  914  915  916  917  918  919  920  921\n",
      "  922  923  924  925  926  927  928  929  930  931  932  933  934  935\n",
      "  936  937  938  939  940  941  942  943  944  945  946  947  948  949\n",
      "  950  951  952  953  954  955  956  957  958  959  960  961  962  963\n",
      "  964  965  966  967  968  969  970  971  972  973  974  975  976  977\n",
      "  978  979  980  981  982  983  984  985  986  987  988  989  990  991\n",
      "  992  993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005\n",
      " 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019\n",
      " 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033\n",
      " 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047\n",
      " 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061\n",
      " 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075\n",
      " 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 3206\n",
      " 3207 3208 3209 3210 3211 3212 3213 3214 3215 3216 3217 3218 3219 3220\n",
      " 3221 3222 3223 3224 3225 3226 3227 3228 3229 3230 3231 3232 3233 3234\n",
      " 3235 3236 3237 3238 3239 3240 3241 3242 3243 3244 3245 3246 3247 3248\n",
      " 3249 3250 3251 3252 3253 3254 3255 3256 3257 3258 3259 3260 3261 3262\n",
      " 3263 3264 3265 3266 3267 3268 3269 3270 3271 3272 3273 3274 3275 3276\n",
      " 3277 3278 3279 3280 3281 3282 3283 3284 3285 3286 3287 3288 3289 3290\n",
      " 3291 3292 3293 3294 3295 3296 3297 3298 3299 3300 3301 3302 3303 3304\n",
      " 3305 3306 3307 3308 3309 3310 3311 3312 3313 3314 3315 3316 3317 3318\n",
      " 3319 3320 3321 3322 3323 3324 3325 3326 3327 3328 3329 3330 3331 3332\n",
      " 3333 3334 3335 3336 3337 3338 3339 3340 3341 3342 3343 3344 3345 3346\n",
      " 3347 3348 3349 3350 3351 3352 3353 3354 3355 3356 3357 3358 3359 3360\n",
      " 3361 3362 3363 3364 3365 3366 3367 3368 3369 3370 3371 3372 3373 3374\n",
      " 3375 3376 3377 3378 3379 3380 3381 3382 3383 3384 3385 3386 3387 3388\n",
      " 3389 3390 3391 3392 3393 3394 3395 3396 3397 3398 3399 3400 3401 3402\n",
      " 3403 3404 3405 3406 3407 3408 3409 3410 3411 3412 3413 3414 3415 3416\n",
      " 3417 3418 3419 3420 3421 3422 3423 3424 3425 3426 3427 3428 3429 3430\n",
      " 3431 3432 3433 3434 3435 3436 3437 3438 3439 3440 3441 3442 3443 3444\n",
      " 3445 3446 3447 3448 3449 3450 3451 3452 3453 3454 3455 3456 3457 3458\n",
      " 3459 3460 3461 3462 3463 3464 3465 3466 3467 3468 3469 3470 3471 3472\n",
      " 3473 3474 3475 3476 3477 3478 3479 3480 3481 3482 3483 3484]\n",
      "[1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102\n",
      " 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116\n",
      " 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130\n",
      " 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144\n",
      " 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158\n",
      " 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172\n",
      " 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186\n",
      " 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200\n",
      " 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214\n",
      " 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228\n",
      " 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242\n",
      " 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256\n",
      " 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 3485\n",
      " 3486 3487 3488 3489 3490 3491 3492 3493 3494 3495 3496 3497 3498 3499\n",
      " 3500 3501 3502 3503 3504 3505 3506 3507 3508 3509 3510 3511 3512 3513\n",
      " 3514 3515 3516 3517 3518 3519 3520 3521 3522 3523 3524 3525 3526 3527\n",
      " 3528 3529 3530 3531 3532 3533 3534 3535 3536 3537 3538 3539 3540 3541\n",
      " 3542 3543 3544 3545 3546 3547 3548 3549 3550 3551 3552 3553 3554 3555\n",
      " 3556 3557 3558 3559 3560 3561 3562 3563 3564 3565 3566 3567 3568 3569\n",
      " 3570 3571 3572 3573 3574 3575 3576 3577 3578 3579 3580 3581 3582 3583\n",
      " 3584 3585 3586 3587 3588 3589 3590 3591 3592 3593 3594 3595 3596 3597\n",
      " 3598 3599 3600 3601 3602 3603 3604 3605 3606 3607 3608 3609 3610 3611\n",
      " 3612 3613 3614 3615 3616 3617 3618 3619 3620 3621 3622 3623 3624 3625\n",
      " 3626 3627 3628 3629 3630 3631 3632 3633 3634 3635 3636 3637 3638 3639\n",
      " 3640 3641 3642 3643 3644 3645 3646 3647 3648 3649 3650 3651 3652 3653\n",
      " 3654 3655 3656 3657 3658 3659 3660 3661 3662 3663 3664 3665 3666 3667\n",
      " 3668 3669 3670 3671 3672 3673 3674 3675 3676 3677 3678 3679 3680 3681\n",
      " 3682 3683 3684 3685 3686 3687 3688 3689 3690 3691 3692 3693 3694 3695\n",
      " 3696 3697 3698 3699 3700 3701 3702 3703 3704 3705 3706 3707 3708 3709\n",
      " 3710 3711 3712 3713 3714 3715 3716 3717 3718 3719 3720 3721 3722 3723\n",
      " 3724 3725 3726 3727 3728 3729 3730 3731 3732 3733 3734 3735 3736 3737\n",
      " 3738 3739 3740 3741 3742 3743 3744 3745 3746 3747 3748 3749 3750 3751\n",
      " 3752 3753 3754 3755 3756 3757 3758 3759 3760 3761 3762 3763]\n",
      "[1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283\n",
      " 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297\n",
      " 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311\n",
      " 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325\n",
      " 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339\n",
      " 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353\n",
      " 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367\n",
      " 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381\n",
      " 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395\n",
      " 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409\n",
      " 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423\n",
      " 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437\n",
      " 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 3764\n",
      " 3765 3766 3767 3768 3769 3770 3771 3772 3773 3774 3775 3776 3777 3778\n",
      " 3779 3780 3781 3782 3783 3784 3785 3786 3787 3788 3789 3790 3791 3792\n",
      " 3793 3794 3795 3796 3797 3798 3799 3800 3801 3802 3803 3804 3805 3806\n",
      " 3807 3808 3809 3810 3811 3812 3813 3814 3815 3816 3817 3818 3819 3820\n",
      " 3821 3822 3823 3824 3825 3826 3827 3828 3829 3830 3831 3832 3833 3834\n",
      " 3835 3836 3837 3838 3839 3840 3841 3842 3843 3844 3845 3846 3847 3848\n",
      " 3849 3850 3851 3852 3853 3854 3855 3856 3857 3858 3859 3860 3861 3862\n",
      " 3863 3864 3865 3866 3867 3868 3869 3870 3871 3872 3873 3874 3875 3876\n",
      " 3877 3878 3879 3880 3881 3882 3883 3884 3885 3886 3887 3888 3889 3890\n",
      " 3891 3892 3893 3894 3895 3896 3897 3898 3899 3900 3901 3902 3903 3904\n",
      " 3905 3906 3907 3908 3909 3910 3911 3912 3913 3914 3915 3916 3917 3918\n",
      " 3919 3920 3921 3922 3923 3924 3925 3926 3927 3928 3929 3930 3931 3932\n",
      " 3933 3934 3935 3936 3937 3938 3939 3940 3941 3942 3943 3944 3945 3946\n",
      " 3947 3948 3949 3950 3951 3952 3953 3954 3955 3956 3957 3958 3959 3960\n",
      " 3961 3962 3963 3964 3965 3966 3967 3968 3969 3970 3971 3972 3973 3974\n",
      " 3975 3976 3977 3978 3979 3980 3981 3982 3983 3984 3985 3986 3987 3988\n",
      " 3989 3990 3991 3992 3993 3994 3995 3996 3997 3998 3999 4000 4001 4002\n",
      " 4003 4004 4005 4006 4007 4008 4009 4010 4011 4012 4013 4014 4015 4016\n",
      " 4017 4018 4019 4020 4021 4022 4023 4024 4025 4026 4027 4028 4029 4030\n",
      " 4031 4032 4033 4034 4035 4036 4037 4038 4039 4040 4041 4042]\n",
      "[1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464\n",
      " 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478\n",
      " 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492\n",
      " 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506\n",
      " 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520\n",
      " 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534\n",
      " 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548\n",
      " 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562\n",
      " 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576\n",
      " 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590\n",
      " 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604\n",
      " 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618\n",
      " 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 4043\n",
      " 4044 4045 4046 4047 4048 4049 4050 4051 4052 4053 4054 4055 4056 4057\n",
      " 4058 4059 4060 4061 4062 4063 4064 4065 4066 4067 4068 4069 4070 4071\n",
      " 4072 4073 4074 4075 4076 4077 4078 4079 4080 4081 4082 4083 4084 4085\n",
      " 4086 4087 4088 4089 4090 4091 4092 4093 4094 4095 4096 4097 4098 4099\n",
      " 4100 4101 4102 4103 4104 4105 4106 4107 4108 4109 4110 4111 4112 4113\n",
      " 4114 4115 4116 4117 4118 4119 4120 4121 4122 4123 4124 4125 4126 4127\n",
      " 4128 4129 4130 4131 4132 4133 4134 4135 4136 4137 4138 4139 4140 4141\n",
      " 4142 4143 4144 4145 4146 4147 4148 4149 4150 4151 4152 4153 4154 4155\n",
      " 4156 4157 4158 4159 4160 4161 4162 4163 4164 4165 4166 4167 4168 4169\n",
      " 4170 4171 4172 4173 4174 4175 4176 4177 4178 4179 4180 4181 4182 4183\n",
      " 4184 4185 4186 4187 4188 4189 4190 4191 4192 4193 4194 4195 4196 4197\n",
      " 4198 4199 4200 4201 4202 4203 4204 4205 4206 4207 4208 4209 4210 4211\n",
      " 4212 4213 4214 4215 4216 4217 4218 4219 4220 4221 4222 4223 4224 4225\n",
      " 4226 4227 4228 4229 4230 4231 4232 4233 4234 4235 4236 4237 4238 4239\n",
      " 4240 4241 4242 4243 4244 4245 4246 4247 4248 4249 4250 4251 4252 4253\n",
      " 4254 4255 4256 4257 4258 4259 4260 4261 4262 4263 4264 4265 4266 4267\n",
      " 4268 4269 4270 4271 4272 4273 4274 4275 4276 4277 4278 4279 4280 4281\n",
      " 4282 4283 4284 4285 4286 4287 4288 4289 4290 4291 4292 4293 4294 4295\n",
      " 4296 4297 4298 4299 4300 4301 4302 4303 4304 4305 4306 4307 4308 4309\n",
      " 4310 4311 4312 4313 4314 4315 4316 4317 4318 4319 4320 4321]\n",
      "[1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645\n",
      " 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659\n",
      " 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673\n",
      " 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687\n",
      " 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701\n",
      " 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715\n",
      " 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729\n",
      " 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743\n",
      " 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757\n",
      " 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771\n",
      " 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785\n",
      " 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799\n",
      " 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 4322\n",
      " 4323 4324 4325 4326 4327 4328 4329 4330 4331 4332 4333 4334 4335 4336\n",
      " 4337 4338 4339 4340 4341 4342 4343 4344 4345 4346 4347 4348 4349 4350\n",
      " 4351 4352 4353 4354 4355 4356 4357 4358 4359 4360 4361 4362 4363 4364\n",
      " 4365 4366 4367 4368 4369 4370 4371 4372 4373 4374 4375 4376 4377 4378\n",
      " 4379 4380 4381 4382 4383 4384 4385 4386 4387 4388 4389 4390 4391 4392\n",
      " 4393 4394 4395 4396 4397 4398 4399 4400 4401 4402 4403 4404 4405 4406\n",
      " 4407 4408 4409 4410 4411 4412 4413 4414 4415 4416 4417 4418 4419 4420\n",
      " 4421 4422 4423 4424 4425 4426 4427 4428 4429 4430 4431 4432 4433 4434\n",
      " 4435 4436 4437 4438 4439 4440 4441 4442 4443 4444 4445 4446 4447 4448\n",
      " 4449 4450 4451 4452 4453 4454 4455 4456 4457 4458 4459 4460 4461 4462\n",
      " 4463 4464 4465 4466 4467 4468 4469 4470 4471 4472 4473 4474 4475 4476\n",
      " 4477 4478 4479 4480 4481 4482 4483 4484 4485 4486 4487 4488 4489 4490\n",
      " 4491 4492 4493 4494 4495 4496 4497 4498 4499 4500 4501 4502 4503 4504\n",
      " 4505 4506 4507 4508 4509 4510 4511 4512 4513 4514 4515 4516 4517 4518\n",
      " 4519 4520 4521 4522 4523 4524 4525 4526 4527 4528 4529 4530 4531 4532\n",
      " 4533 4534 4535 4536 4537 4538 4539 4540 4541 4542 4543 4544 4545 4546\n",
      " 4547 4548 4549 4550 4551 4552 4553 4554 4555 4556 4557 4558 4559 4560\n",
      " 4561 4562 4563 4564 4565 4566 4567 4568 4569 4570 4571 4572 4573 4574\n",
      " 4575 4576 4577 4578 4579 4580 4581 4582 4583 4584 4585 4586 4587 4588\n",
      " 4589 4590 4591 4592 4593 4594 4595 4596 4597 4598 4599 4600]\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=72)\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    print(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [],
   "name": "Large_Margin_Perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
